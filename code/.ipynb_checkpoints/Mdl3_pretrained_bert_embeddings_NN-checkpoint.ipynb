{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fff3601",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6decb56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 02:54:26.867541: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/nitesh/env/dev38/python38/lib/python3.8/site-packages/xgboost/compat.py:93: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import BertModel\n",
    "from transformers import AutoTokenizer,AutoModel\n",
    "from torch.utils.data import Dataset\n",
    "# from transformers import AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "print(transformers.__version__)\n",
    "# !pip install evaluate\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc42a123",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80238235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim Categories: ['property' 'auto' 'health']\n",
      "Training dataset size: 24000\n",
      "Test dataset size: 6000\n",
      "Training data Examples: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32057/2093047390.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[\"encoded_label\"] = labelencoder.fit_transform(train['label'])\n",
      "/tmp/ipykernel_32057/2093047390.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test[\"encoded_label\"] = labelencoder.transform(test['label'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>policy_num</th>\n",
       "      <th>patient_name</th>\n",
       "      <th>date</th>\n",
       "      <th>policy_claim_no</th>\n",
       "      <th>text</th>\n",
       "      <th>claim_amount</th>\n",
       "      <th>claim_status</th>\n",
       "      <th>communication_history</th>\n",
       "      <th>encoded_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21753</th>\n",
       "      <td>auto</td>\n",
       "      <td>151393</td>\n",
       "      <td>Ava Williams</td>\n",
       "      <td>2023-02-15</td>\n",
       "      <td>9433</td>\n",
       "      <td>There wa an incident on 2023-02-15 where Vehic...</td>\n",
       "      <td>2851</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>We need to ass the impact and consequence of t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>health</td>\n",
       "      <td>610541</td>\n",
       "      <td>Elijah Johnson</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>9100</td>\n",
       "      <td>There wa a critical sprain incident on 2023-03...</td>\n",
       "      <td>25912</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>We need to establish preventive measure to avo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22941</th>\n",
       "      <td>property</td>\n",
       "      <td>651336</td>\n",
       "      <td>Mia Martin</td>\n",
       "      <td>2022-10-23</td>\n",
       "      <td>5130</td>\n",
       "      <td>A significant incident took place on 2022-10-2...</td>\n",
       "      <td>94044</td>\n",
       "      <td>Approved</td>\n",
       "      <td>I am seeking reimbursement for the replacement...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>property</td>\n",
       "      <td>721591</td>\n",
       "      <td>Olivia Allen</td>\n",
       "      <td>2023-04-18</td>\n",
       "      <td>7523</td>\n",
       "      <td>A severe [ Property Incident 11 ] took place o...</td>\n",
       "      <td>55573</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>I would appreciate your guidance in understand...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17090</th>\n",
       "      <td>auto</td>\n",
       "      <td>523298</td>\n",
       "      <td>Alexander White</td>\n",
       "      <td>2022-12-07</td>\n",
       "      <td>2030</td>\n",
       "      <td>On 2022-12-07 , a minor collision occurred inv...</td>\n",
       "      <td>57898</td>\n",
       "      <td>Denied</td>\n",
       "      <td>Please gather information on the impact of the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29802</th>\n",
       "      <td>health</td>\n",
       "      <td>851474</td>\n",
       "      <td>Daniel Hall</td>\n",
       "      <td>2022-12-11</td>\n",
       "      <td>1044</td>\n",
       "      <td>There wa a motor vehicle accident on 2022-12-1...</td>\n",
       "      <td>84412</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>I request a thorough investigation into the fa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>property</td>\n",
       "      <td>831608</td>\n",
       "      <td>Sebastian Williams</td>\n",
       "      <td>2023-01-15</td>\n",
       "      <td>5309</td>\n",
       "      <td>A severe [ Property Incident 30 ] took place o...</td>\n",
       "      <td>65063</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>I request a detailed breakdown of the coverage...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>auto</td>\n",
       "      <td>682288</td>\n",
       "      <td>Ava Davis</td>\n",
       "      <td>2022-07-04</td>\n",
       "      <td>5500</td>\n",
       "      <td>On 2022-07-04 , a minor collision occurred inv...</td>\n",
       "      <td>73324</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>I would like to discus any legal implication o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>auto</td>\n",
       "      <td>645125</td>\n",
       "      <td>James Lee</td>\n",
       "      <td>2022-08-31</td>\n",
       "      <td>4689</td>\n",
       "      <td>A tree falling incident took place on 2022-08-...</td>\n",
       "      <td>8726</td>\n",
       "      <td>Denied</td>\n",
       "      <td>I request a thorough analysis of the policy an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23654</th>\n",
       "      <td>property</td>\n",
       "      <td>789355</td>\n",
       "      <td>Benjamin Miller</td>\n",
       "      <td>2022-11-02</td>\n",
       "      <td>3819</td>\n",
       "      <td>There wa a significant [ Property Incident 17 ...</td>\n",
       "      <td>44084</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>I request a detailed breakdown of the coverage...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          label  policy_num        patient_name        date  policy_claim_no  \\\n",
       "21753      auto      151393        Ava Williams  2023-02-15             9433   \n",
       "251      health      610541      Elijah Johnson  2023-03-01             9100   \n",
       "22941  property      651336          Mia Martin  2022-10-23             5130   \n",
       "618    property      721591        Olivia Allen  2023-04-18             7523   \n",
       "17090      auto      523298     Alexander White  2022-12-07             2030   \n",
       "...         ...         ...                 ...         ...              ...   \n",
       "29802    health      851474         Daniel Hall  2022-12-11             1044   \n",
       "5390   property      831608  Sebastian Williams  2023-01-15             5309   \n",
       "860        auto      682288           Ava Davis  2022-07-04             5500   \n",
       "15795      auto      645125           James Lee  2022-08-31             4689   \n",
       "23654  property      789355     Benjamin Miller  2022-11-02             3819   \n",
       "\n",
       "                                                    text  claim_amount  \\\n",
       "21753  There wa an incident on 2023-02-15 where Vehic...          2851   \n",
       "251    There wa a critical sprain incident on 2023-03...         25912   \n",
       "22941  A significant incident took place on 2022-10-2...         94044   \n",
       "618    A severe [ Property Incident 11 ] took place o...         55573   \n",
       "17090  On 2022-12-07 , a minor collision occurred inv...         57898   \n",
       "...                                                  ...           ...   \n",
       "29802  There wa a motor vehicle accident on 2022-12-1...         84412   \n",
       "5390   A severe [ Property Incident 30 ] took place o...         65063   \n",
       "860    On 2022-07-04 , a minor collision occurred inv...         73324   \n",
       "15795  A tree falling incident took place on 2022-08-...          8726   \n",
       "23654  There wa a significant [ Property Incident 17 ...         44084   \n",
       "\n",
       "      claim_status                              communication_history  \\\n",
       "21753  In Progress  We need to ass the impact and consequence of t...   \n",
       "251    In Progress  We need to establish preventive measure to avo...   \n",
       "22941     Approved  I am seeking reimbursement for the replacement...   \n",
       "618    In Progress  I would appreciate your guidance in understand...   \n",
       "17090       Denied  Please gather information on the impact of the...   \n",
       "...            ...                                                ...   \n",
       "29802  In Progress  I request a thorough investigation into the fa...   \n",
       "5390   In Progress  I request a detailed breakdown of the coverage...   \n",
       "860    In Progress  I would like to discus any legal implication o...   \n",
       "15795       Denied  I request a thorough analysis of the policy an...   \n",
       "23654  In Progress  I request a detailed breakdown of the coverage...   \n",
       "\n",
       "       encoded_label  \n",
       "21753              0  \n",
       "251                1  \n",
       "22941              2  \n",
       "618                2  \n",
       "17090              0  \n",
       "...              ...  \n",
       "29802              1  \n",
       "5390               2  \n",
       "860                0  \n",
       "15795              0  \n",
       "23654              2  \n",
       "\n",
       "[24000 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data Examples: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>policy_num</th>\n",
       "      <th>patient_name</th>\n",
       "      <th>date</th>\n",
       "      <th>policy_claim_no</th>\n",
       "      <th>text</th>\n",
       "      <th>claim_amount</th>\n",
       "      <th>claim_status</th>\n",
       "      <th>communication_history</th>\n",
       "      <th>encoded_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>property</td>\n",
       "      <td>283620</td>\n",
       "      <td>Charlotte Miller</td>\n",
       "      <td>2023-03-12</td>\n",
       "      <td>4716</td>\n",
       "      <td>An alarming incident involving a shopping mall...</td>\n",
       "      <td>4978</td>\n",
       "      <td>Approved</td>\n",
       "      <td>I am seeking reimbursement for the repair and ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22404</th>\n",
       "      <td>health</td>\n",
       "      <td>439665</td>\n",
       "      <td>Charlotte Anderson</td>\n",
       "      <td>2023-01-17</td>\n",
       "      <td>7540</td>\n",
       "      <td>On 2023-01-17 , Charlotte Anderson encountered...</td>\n",
       "      <td>39122</td>\n",
       "      <td>Pending</td>\n",
       "      <td>Please provide additional information regardin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23397</th>\n",
       "      <td>property</td>\n",
       "      <td>707259</td>\n",
       "      <td>Daniel Williams</td>\n",
       "      <td>2023-04-08</td>\n",
       "      <td>2917</td>\n",
       "      <td>A distressing [ Property Incident 34 ] took pl...</td>\n",
       "      <td>93664</td>\n",
       "      <td>Denied</td>\n",
       "      <td>I need assistance with documenting and itemizi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25058</th>\n",
       "      <td>health</td>\n",
       "      <td>326420</td>\n",
       "      <td>Amelia Allen</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>8716</td>\n",
       "      <td>There wa a significant puncture incident on 20...</td>\n",
       "      <td>90715</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>Kindly provide a detailed account of the healt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2664</th>\n",
       "      <td>property</td>\n",
       "      <td>741696</td>\n",
       "      <td>Jack Walker</td>\n",
       "      <td>2023-06-02</td>\n",
       "      <td>8965</td>\n",
       "      <td>A distressing incident took place on 2023-06-0...</td>\n",
       "      <td>55454</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>I am writing to report a property incident and...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2210</th>\n",
       "      <td>property</td>\n",
       "      <td>462779</td>\n",
       "      <td>Charlotte Jackson</td>\n",
       "      <td>2023-01-24</td>\n",
       "      <td>1471</td>\n",
       "      <td>On 2023-01-24 , a critical [ Property Incident...</td>\n",
       "      <td>9732</td>\n",
       "      <td>Approved</td>\n",
       "      <td>I need to submit a property insurance claim du...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14144</th>\n",
       "      <td>health</td>\n",
       "      <td>534443</td>\n",
       "      <td>William Allen</td>\n",
       "      <td>2022-08-21</td>\n",
       "      <td>4690</td>\n",
       "      <td>There wa a car accident incident on 2022-08-21...</td>\n",
       "      <td>7388</td>\n",
       "      <td>Denied</td>\n",
       "      <td>Kindly provide a detailed account of the healt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23108</th>\n",
       "      <td>health</td>\n",
       "      <td>750239</td>\n",
       "      <td>Elijah White</td>\n",
       "      <td>2022-09-29</td>\n",
       "      <td>7323</td>\n",
       "      <td>There wa a heart attack incident on 2022-09-29...</td>\n",
       "      <td>30633</td>\n",
       "      <td>Pending</td>\n",
       "      <td>Kindly provide a detailed account of the healt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25703</th>\n",
       "      <td>auto</td>\n",
       "      <td>252408</td>\n",
       "      <td>Sebastian Jackson</td>\n",
       "      <td>2023-03-12</td>\n",
       "      <td>7733</td>\n",
       "      <td>On 2023-03-12 , Vehicle W encountered a mechan...</td>\n",
       "      <td>59718</td>\n",
       "      <td>Approved</td>\n",
       "      <td>I request a comprehensive report on the auto i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29171</th>\n",
       "      <td>health</td>\n",
       "      <td>372671</td>\n",
       "      <td>Sophia Jackson</td>\n",
       "      <td>2022-07-15</td>\n",
       "      <td>1753</td>\n",
       "      <td>A significant spinal injury incident took plac...</td>\n",
       "      <td>52115</td>\n",
       "      <td>Denied</td>\n",
       "      <td>It is essential to gather all pertinent inform...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          label  policy_num        patient_name        date  policy_claim_no  \\\n",
       "2308   property      283620    Charlotte Miller  2023-03-12             4716   \n",
       "22404    health      439665  Charlotte Anderson  2023-01-17             7540   \n",
       "23397  property      707259     Daniel Williams  2023-04-08             2917   \n",
       "25058    health      326420        Amelia Allen  2023-05-08             8716   \n",
       "2664   property      741696         Jack Walker  2023-06-02             8965   \n",
       "...         ...         ...                 ...         ...              ...   \n",
       "2210   property      462779   Charlotte Jackson  2023-01-24             1471   \n",
       "14144    health      534443       William Allen  2022-08-21             4690   \n",
       "23108    health      750239        Elijah White  2022-09-29             7323   \n",
       "25703      auto      252408   Sebastian Jackson  2023-03-12             7733   \n",
       "29171    health      372671      Sophia Jackson  2022-07-15             1753   \n",
       "\n",
       "                                                    text  claim_amount  \\\n",
       "2308   An alarming incident involving a shopping mall...          4978   \n",
       "22404  On 2023-01-17 , Charlotte Anderson encountered...         39122   \n",
       "23397  A distressing [ Property Incident 34 ] took pl...         93664   \n",
       "25058  There wa a significant puncture incident on 20...         90715   \n",
       "2664   A distressing incident took place on 2023-06-0...         55454   \n",
       "...                                                  ...           ...   \n",
       "2210   On 2023-01-24 , a critical [ Property Incident...          9732   \n",
       "14144  There wa a car accident incident on 2022-08-21...          7388   \n",
       "23108  There wa a heart attack incident on 2022-09-29...         30633   \n",
       "25703  On 2023-03-12 , Vehicle W encountered a mechan...         59718   \n",
       "29171  A significant spinal injury incident took plac...         52115   \n",
       "\n",
       "      claim_status                              communication_history  \\\n",
       "2308      Approved  I am seeking reimbursement for the repair and ...   \n",
       "22404      Pending  Please provide additional information regardin...   \n",
       "23397       Denied  I need assistance with documenting and itemizi...   \n",
       "25058  In Progress  Kindly provide a detailed account of the healt...   \n",
       "2664   In Progress  I am writing to report a property incident and...   \n",
       "...            ...                                                ...   \n",
       "2210      Approved  I need to submit a property insurance claim du...   \n",
       "14144       Denied  Kindly provide a detailed account of the healt...   \n",
       "23108      Pending  Kindly provide a detailed account of the healt...   \n",
       "25703     Approved  I request a comprehensive report on the auto i...   \n",
       "29171       Denied  It is essential to gather all pertinent inform...   \n",
       "\n",
       "       encoded_label  \n",
       "2308               2  \n",
       "22404              1  \n",
       "23397              2  \n",
       "25058              1  \n",
       "2664               2  \n",
       "...              ...  \n",
       "2210               2  \n",
       "14144              1  \n",
       "23108              1  \n",
       "25703              0  \n",
       "29171              1  \n",
       "\n",
       "[6000 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df = pd.read_csv('insurance_synthetic_data.csv')\n",
    "df  = pd.read_csv('../dataset/insurance_claims_data/insurance_claims.csv')\n",
    "df = df.rename(columns = {'incident_class':'label'})\n",
    "df = df.rename(columns = {'incident_description':'text'})\n",
    "# display(df)\n",
    "print(\"Claim Categories:\", df.label.unique())\n",
    "\n",
    "\n",
    "# Divide the data into train, validation, and test sets\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "# val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Print the sizes of the datasets\n",
    "print(\"Training dataset size:\", len(train))\n",
    "print(\"Test dataset size:\", len(test))\n",
    "\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "labelencoder.fit(train['label'])\n",
    "train[\"encoded_label\"] = labelencoder.fit_transform(train['label'])\n",
    "test[\"encoded_label\"] = labelencoder.transform(test['label'])\n",
    "print(\"Training data Examples: \")\n",
    "display(train)\n",
    "print(\"Test data Examples: \")\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cc4cba",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "960404cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert your train and test text data to lists\n",
    "train_texts = train[\"text\"].tolist()\n",
    "test_texts = test[\"text\"].tolist()\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train_encodings = tokenizer(\n",
    "    train_texts,\n",
    "    padding=True,\n",
    "    max_length=12,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "test_encodings = tokenizer(\n",
    "    test_texts,\n",
    "    padding=True,\n",
    "    max_length=12,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48782e3c",
   "metadata": {},
   "source": [
    "## Generate Pretrained BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c2c9e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Device: NVIDIA GeForce RTX 3070 Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/tmp/ipykernel_32057/3768980045.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_tokens = {key: torch.tensor(val).to(device) for key, val in batch_tokens.items()}\n",
      "/tmp/ipykernel_32057/3768980045.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_tokens = {key: torch.tensor(val).to(device) for key, val in batch_tokens.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape:  torch.Size([19200, 768]) validation data shape:  torch.Size([4800, 768]) test embd shape: torch.Size([6000, 768])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "train_embeddings = []\n",
    "test_embeddings = []\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Get the GPU device on which we run our experiments\n",
    "gpu_name = torch.cuda.get_device_name(device) if device.type == 'cuda' else 'N/A'\n",
    "print(\"GPU Device:\", gpu_name)\n",
    "\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model = model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(train_encodings['input_ids']), batch_size):\n",
    "        batch_tokens = {key: val[i:i+batch_size] for key, val in train_encodings.items()}\n",
    "        batch_tokens = {key: torch.tensor(val).to(device) for key, val in batch_tokens.items()}\n",
    "        batch_outputs = model(**batch_tokens)\n",
    "        batch_embeddings = batch_outputs.last_hidden_state[:, 0, :].to('cpu')  # Move back to CPU temporarily\n",
    "        train_embeddings.append(batch_embeddings.to(device))  # Move to CUDA device\n",
    "\n",
    "    for i in range(0, len(test_encodings['input_ids']), batch_size):\n",
    "        batch_tokens = {key: val[i:i+batch_size] for key, val in test_encodings.items()}\n",
    "        batch_tokens = {key: torch.tensor(val).to(device) for key, val in batch_tokens.items()}\n",
    "        batch_outputs = model(**batch_tokens)\n",
    "        batch_embeddings = batch_outputs.last_hidden_state[:, 0, :].to('cpu')  # Move back to CPU temporarily\n",
    "        test_embeddings.append(batch_embeddings.to(device))  # Move to CUDA device\n",
    "\n",
    "train_embeddings = torch.cat(train_embeddings, dim=0).to('cpu')  # Move back to CPU\n",
    "test_embeddings = torch.cat(test_embeddings, dim=0).to('cpu')  # Move back to CPU\n",
    "\n",
    "\n",
    "\n",
    "train_labels=train.encoded_label.to_list()\n",
    "test_labels=test.encoded_label.to_list()\n",
    "\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "train_embeddings1, val_embeddings1, train_labels1, val_labels1 = train_test_split(train_embeddings, \n",
    "                                                                                  train_labels, \n",
    "                                                                                  test_size=0.2, \n",
    "                                                                                  random_state=42)\n",
    "print(\"train data shape: \", train_embeddings1.shape, \"validation data shape: \", val_embeddings1.shape,\n",
    "     'test embd shape:', test_embeddings.shape)\n",
    "      \n",
    "# print('train labels shape: ', len(train_labels1), \"validation labels shape\", len(val_labels1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4a95129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 0.1887\n",
      "Epoch 1/5, Validation Loss: 0.1052\n",
      "Epoch 2/5, Train Loss: 0.0886\n",
      "Epoch 2/5, Validation Loss: 0.0777\n",
      "Epoch 3/5, Train Loss: 0.0831\n",
      "Epoch 3/5, Validation Loss: 0.0744\n",
      "Epoch 4/5, Train Loss: 0.0850\n",
      "Epoch 4/5, Validation Loss: 0.0789\n",
      "Epoch 5/5, Train Loss: 0.0831\n",
      "Epoch 5/5, Validation Loss: 0.0737\n",
      "Training Data Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.942     0.921     0.932      6512\n",
      "           1      0.922     0.964     0.943      6346\n",
      "           2      1.000     0.978     0.989      6342\n",
      "\n",
      "    accuracy                          0.954     19200\n",
      "   macro avg      0.955     0.954     0.954     19200\n",
      "weighted avg      0.955     0.954     0.954     19200\n",
      "\n",
      "Validation Data Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.944     0.931     0.938      1628\n",
      "           1      0.933     0.968     0.950      1608\n",
      "           2      1.000     0.976     0.988      1564\n",
      "\n",
      "    accuracy                          0.958      4800\n",
      "   macro avg      0.959     0.958     0.958      4800\n",
      "weighted avg      0.958     0.958     0.958      4800\n",
      "\n",
      "Test Data Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.943     0.935     0.939      2040\n",
      "           1      0.935     0.965     0.950      1992\n",
      "           2      1.000     0.977     0.988      1968\n",
      "\n",
      "    accuracy                          0.959      6000\n",
      "   macro avg      0.959     0.959     0.959      6000\n",
      "weighted avg      0.959     0.959     0.959      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the multiclass classification model\n",
    "class MulticlassClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(MulticlassClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model_NN = MulticlassClassifier(input_size=train_embeddings1.shape[1], num_classes=len(labelencoder.classes_))\n",
    "model_NN = model_NN.to(device)  # Move model to CUDA GPU\n",
    "optimizer = optim.Adam(model_NN.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 5\n",
    "batch_size = 16\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for i in range(0, len(train_embeddings1), batch_size):\n",
    "        batch_embeddings = train_embeddings1[i:i+batch_size].requires_grad_(True).to(device)  # Move tensor to CUDA GPU\n",
    "        batch_labels = train_labels1[i:i+batch_size]\n",
    "        batch_labels = torch.tensor(batch_labels).to(device).long()  # Convert to PyTorch tensor and move to CUDA GPU\n",
    "        \n",
    "        # Forward pass\n",
    "        train_outputs = model_NN(batch_embeddings)\n",
    "        loss = criterion(train_outputs, batch_labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()  # Accumulate the loss\n",
    "        \n",
    "    avg_train_loss = epoch_loss / (len(train_embeddings1) // batch_size)  # Calculate average training loss\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "\n",
    "    # Evaluate the model on validation set\n",
    "    with torch.no_grad():\n",
    "        val_embeddings2 = val_embeddings1.to(device)  # Move tensor to CUDA GPU\n",
    "        val_labels2 = torch.tensor(val_labels1).clone().detach().to(device).long()  # Convert to PyTorch tensor and move to CUDA GPU\n",
    "        val_outputs = model_NN(val_embeddings2)\n",
    "        val_loss = criterion(val_outputs, val_labels2)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss.item():.4f}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# # Convert the predicted values and original test labels to CPU\n",
    "# train_preds = train_outputs.to('cpu').argmax(dim=1).numpy()\n",
    "# val_labels = val_labels1.to('cpu').numpy()\n",
    "\n",
    "# # Generate classification report\n",
    "# print(\"Validation Data Classification Report:\")\n",
    "# classification_rep_val = classification_report(val_labels, val_preds)\n",
    "# print(classification_rep_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_NN.eval()\n",
    "with torch.no_grad():\n",
    "    train_emb = train_embeddings1.to(device)  # Move tensor to CUDA GPU\n",
    "    train_lbls = torch.tensor(train_labels1).clone().detach().to(device).long() # Convert to PyTorch tensor and move to CUDA GPU\n",
    "    train_otpt = model_NN(train_emb)\n",
    "#     test_loss = criterion(val_otpt, val_lbls)    \n",
    "\n",
    "# Convert the predicted values and original test labels to CPU\n",
    "train_preds = train_otpt.to('cpu').argmax(dim=1).numpy()\n",
    "\n",
    "# val_labels = val_labels1.to('cpu').numpy()\n",
    "train_labels=np.array(train_labels1)\n",
    "\n",
    "# Generate classification report\n",
    "print(\"Training Data Classification Report:\")\n",
    "classification_rep_train = classification_report(train_labels, train_preds,digits=3)\n",
    "print(classification_rep_train)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# Evaluate the model on test set\n",
    "model_NN.eval()\n",
    "with torch.no_grad():\n",
    "    val_emb = val_embeddings1.to(device)  # Move tensor to CUDA GPU\n",
    "    val_lbls = torch.tensor(val_labels1).clone().detach().to(device).long() # Convert to PyTorch tensor and move to CUDA GPU\n",
    "    val_otpt = model_NN(val_emb)\n",
    "#     test_loss = criterion(val_otpt, val_lbls)    \n",
    "\n",
    "# Convert the predicted values and original test labels to CPU\n",
    "val_preds = val_otpt.to('cpu').argmax(dim=1).numpy()\n",
    "\n",
    "# val_labels = val_labels1.to('cpu').numpy()\n",
    "val_labels=np.array(val_labels1)\n",
    "\n",
    "# Generate classification report\n",
    "print(\"Validation Data Classification Report:\")\n",
    "classification_rep_val = classification_report(val_labels, val_preds, digits=3)\n",
    "print(classification_rep_val)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "# Evaluate the model on test set\n",
    "model_NN.eval()\n",
    "with torch.no_grad():\n",
    "    test_embeddings = test_embeddings.to(device)  # Move tensor to CUDA GPU\n",
    "    test_labels = torch.tensor(test_labels).to(device).long()  # Convert to PyTorch tensor and move to CUDA GPU\n",
    "    test_outputs = model_NN(test_embeddings)\n",
    "    test_loss = criterion(test_outputs, test_labels)\n",
    "\n",
    "# Convert the predicted values and original test labels to CPU\n",
    "test_preds = test_outputs.to('cpu').argmax(dim=1).numpy()\n",
    "test_labels = test_labels.to('cpu').numpy()\n",
    "\n",
    "print(\"Test Data Classification Report:\")\n",
    "# Generate classification report\n",
    "classification_rep_test = classification_report(test_labels, test_preds, digits=3)\n",
    "print(classification_rep_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe15327",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
