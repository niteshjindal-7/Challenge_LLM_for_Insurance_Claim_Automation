{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0891cf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import t #fetch t values for t-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80cb456e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models Performance on Test data Table: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>EvalSet</th>\n",
       "      <th>M-Precision</th>\n",
       "      <th>M-Recall</th>\n",
       "      <th>M-F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pretrained_BERTEmbd_with_NN</td>\n",
       "      <td>test</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pretrained_BERTEmbd_with_NN</td>\n",
       "      <td>test</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pretrained_BERTEmbd_with_NN</td>\n",
       "      <td>test</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pretrained_BERTEmbd_with_NN</td>\n",
       "      <td>test</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pretrained_BERTEmbd_with_NN</td>\n",
       "      <td>test</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BERT_base_uncased</td>\n",
       "      <td>test</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BERT_base_uncased</td>\n",
       "      <td>test</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BERT_base_uncased</td>\n",
       "      <td>test</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BERT_base_uncased</td>\n",
       "      <td>test</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BERT_base_uncased</td>\n",
       "      <td>test</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Pretrained_BERTEmbd_with_XGB</td>\n",
       "      <td>test</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Pretrained_BERTEmbd_with_XGB</td>\n",
       "      <td>test</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Pretrained_BERTEmbd_with_XGB</td>\n",
       "      <td>test</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Pretrained_BERTEmbd_with_XGB</td>\n",
       "      <td>test</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Pretrained_BERTEmbd_with_XGB</td>\n",
       "      <td>test</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Model EvalSet  M-Precision  M-Recall  M-F1-Score\n",
       "0    Pretrained_BERTEmbd_with_NN    test        0.959     0.958       0.959\n",
       "1    Pretrained_BERTEmbd_with_NN    test        0.952     0.946       0.946\n",
       "2    Pretrained_BERTEmbd_with_NN    test        0.953     0.952       0.951\n",
       "3    Pretrained_BERTEmbd_with_NN    test        0.955     0.953       0.954\n",
       "4    Pretrained_BERTEmbd_with_NN    test        0.956     0.955       0.955\n",
       "5              BERT_base_uncased    test        0.959     0.954       0.954\n",
       "6              BERT_base_uncased    test        0.959     0.953       0.954\n",
       "7              BERT_base_uncased    test        0.956     0.947       0.948\n",
       "8              BERT_base_uncased    test        0.955     0.953       0.954\n",
       "9              BERT_base_uncased    test        0.958     0.950       0.951\n",
       "10  Pretrained_BERTEmbd_with_XGB    test        0.956     0.956       0.956\n",
       "11  Pretrained_BERTEmbd_with_XGB    test        0.953     0.953       0.953\n",
       "12  Pretrained_BERTEmbd_with_XGB    test        0.953     0.952       0.953\n",
       "13  Pretrained_BERTEmbd_with_XGB    test        0.951     0.950       0.951\n",
       "14  Pretrained_BERTEmbd_with_XGB    test        0.952     0.951       0.951"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-table score 2.776\n",
      "\n",
      "M-F-1 Difference from Base model Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>S1</th>\n",
       "      <th>S100</th>\n",
       "      <th>S500</th>\n",
       "      <th>S1000</th>\n",
       "      <th>S1500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERT_base_uncased</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pretrained_BERTEmbd_with_NN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pretrained_BERTEmbd_with_XGB</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model   S1  S100  S500  S1000  S1500\n",
       "0             BERT_base_uncased  0.2   0.1   0.5    0.3    0.0\n",
       "1   Pretrained_BERTEmbd_with_NN  0.3   0.7   0.2    0.3    0.4\n",
       "2  Pretrained_BERTEmbd_with_XGB  0.0   0.0   0.0    0.0    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21278/4185373709.py:44: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  t_stats = (row['Mean'] * np.sqrt(samples))/row['StdDev']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>t_stats</th>\n",
       "      <th>HYPOTHESIS_CHECK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERT_base_uncased</td>\n",
       "      <td>2.557448</td>\n",
       "      <td>Accept_Null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pretrained_BERTEmbd_with_NN</td>\n",
       "      <td>4.417410</td>\n",
       "      <td>Reject_Null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pretrained_BERTEmbd_with_XGB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model   t_stats HYPOTHESIS_CHECK\n",
       "0             BERT_base_uncased  2.557448      Accept_Null\n",
       "1   Pretrained_BERTEmbd_with_NN  4.417410      Reject_Null\n",
       "2  Pretrained_BERTEmbd_with_XGB       NaN              nan"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set display options to show all columns and unlimited column width\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "\n",
    "directory='../result_xls/'\n",
    "# Filter files with .xlsx extension\n",
    "xlsx_files = [file for file in os.listdir(directory) if file.endswith('.xlsx')]\n",
    "\n",
    "\n",
    "# Read each Excel file into a DataFrame\n",
    "dataframes = []\n",
    "for file in xlsx_files:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    df = pd.read_excel(file_path)\n",
    "    # Extract file name without extension\n",
    "    model_name = os.path.splitext(file)[0]\n",
    "    # Update \"Model\" column with file name\n",
    "    df['Model'] = model_name\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(dataframes)\n",
    "combined_df=combined_df[['Model','EvalSet', 'M-Precision', 'M-Recall', 'M-F1-Score']]\n",
    "combined_df\n",
    "combined_df = combined_df.reset_index(drop=True)\n",
    "\n",
    "filtered_df = combined_df[combined_df['EvalSet'] == 'test'].reset_index(drop=True)\n",
    "print(\"Models Performance on Test data Table: \")\n",
    "display(filtered_df)\n",
    "\n",
    "\n",
    "def calculate_stddev(row, samples=5):\n",
    "    deviations = row[1:-1] - row[-1]  # Subtract mean from each value\n",
    "    squared_deviations = deviations ** 2  # Square the deviations\n",
    "    sum_squared_deviations = np.sum(squared_deviations)  # Calculate mean of squared deviations\n",
    "    stddev = np.sqrt(sum_squared_deviations/(samples-1))\n",
    "    \n",
    "    return stddev\n",
    "\n",
    "# Define a function to calculate t-statistics\n",
    "def calculate_t_stats(row, samples=5):\n",
    "    t_stats = (row['Mean'] * np.sqrt(samples))/row['StdDev']\n",
    "    return t_stats\n",
    "\n",
    "\n",
    "p_val=0.05\n",
    "samples=5\n",
    "degrees_of_freedom= samples-1\n",
    "\n",
    "# Calculate the t-value\n",
    "t_tbl_scr = round(t.ppf(1 - p_val/2, degrees_of_freedom),3)\n",
    "print('t-table score', t_tbl_scr)\n",
    "# https://towardsdatascience.com/paired-t-test-to-evaluate-machine-learning-classifiers-1f395a6c93fa\n",
    "\n",
    "\n",
    "###############################\n",
    "## PAIRED TTEST on M-F1 Score\n",
    "###############################\n",
    "\n",
    "sample =list([1,100, 500, 1000, 1500])\n",
    "filtered_df1 = filtered_df[['Model', 'M-F1-Score']].copy()\n",
    "no_of_mdls = len(filtered_df1.Model.unique())\n",
    "no_of_mdls\n",
    "\n",
    "filtered_df1['sample_seed'] = sample * no_of_mdls\n",
    "filtered_df1\n",
    "\n",
    "pvt_filtered_df1 = filtered_df1.pivot(index='Model', columns='sample_seed', values='M-F1-Score')\n",
    "pvt_filtered_df1 = pvt_filtered_df1 .reset_index()\n",
    "pvt_filtered_df1\n",
    "\n",
    "column_mapping = {1: 'S1', 100: 'S100', 500: 'S500', 1000: 'S1000', 1500: 'S1500'}\n",
    "pvt_filtered_df2 = pvt_filtered_df1.rename(columns=column_mapping).rename_axis(columns=None)\n",
    "pvt_filtered_df2\n",
    "\n",
    "\n",
    "\n",
    "temp=pvt_filtered_df2.iloc[:, 1:].mul(100)\n",
    "temp1= pd.concat([pvt_filtered_df2['Model'], temp], axis=1)\n",
    "\n",
    "\n",
    "# Set the 'Model' column as the index\n",
    "temp1.set_index('Model', inplace=True)\n",
    "\n",
    "# Select the row for subtraction\n",
    "subtract_from = temp1.loc['Pretrained_BERTEmbd_with_XGB']\n",
    "\n",
    "\n",
    "# Subtract the selected row from all other rows\n",
    "df_subtracted = temp1.subtract(subtract_from).abs()\n",
    "\n",
    "# Reset the index\n",
    "df_subtracted.reset_index(inplace=True)\n",
    "\n",
    "# Print the modified DataFrame\n",
    "print()\n",
    "print('M-F-1 Difference from Base model Table:')\n",
    "display(df_subtracted)\n",
    "\n",
    "\n",
    "###################################\n",
    "## Accept or Reject Null Hypothesis\n",
    "###################################\n",
    "\n",
    "# Calculate Mean\n",
    "df_subtracted['Mean'] = df_subtracted.iloc[:, 1:].sum(axis=1) / 5\n",
    "\n",
    "\n",
    "# Apply the function to each row and assign the results to the 'StdDev' column\n",
    "df_subtracted['StdDev'] = df_subtracted.apply(calculate_stddev, axis=1)\n",
    "\n",
    "\n",
    "# Apply the function to each row and assign the results to the 't_stats' column\n",
    "df_subtracted['t_stats'] = df_subtracted.apply(calculate_t_stats, axis=1)\n",
    "\n",
    "\n",
    "# # import numpy as np\n",
    "\n",
    "df_subtracted['HYPOTHESIS_CHECK'] = np.where(df_subtracted['t_stats'].isna(), np.nan, \n",
    "                                             np.where((-2.776 <= df_subtracted['t_stats']) & \n",
    "                                                      (df_subtracted['t_stats'] <= 2.776), \n",
    "                                                      'Accept_Null', 'Reject_Null'))\n",
    "\n",
    "\n",
    "df_subtracted = df_subtracted[['Model', 't_stats', 'HYPOTHESIS_CHECK']]\n",
    "# #Accept NULL = No significance difference between two classifiers. \n",
    "display(df_subtracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0452bf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subtracted.to_excel(f'../result_xls/mdl_compr_ttest_result/model_comparison_ttest.xlsx',\n",
    "                       index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa8b01e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
