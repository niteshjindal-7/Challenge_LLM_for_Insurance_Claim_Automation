{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59af12b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-01 16:19:35.023314: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import BertModel\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "# from transformers import AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "print(transformers.__version__)\n",
    "# !pip install evaluate\n",
    "import evaluate\n",
    "import os\n",
    "import natsort\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9ae923c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['auto' 'property' 'health']\n",
      "Train set size: 24000\n",
      "Test set size: 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11791/434348283.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['encoded_label'] = labelencoder.fit_transform(train['label'])\n",
      "/tmp/ipykernel_11791/434348283.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test[\"encoded_label\"] = labelencoder.transform(test['label'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>policy_num</th>\n",
       "      <th>patient_name</th>\n",
       "      <th>date</th>\n",
       "      <th>policy_claim_no</th>\n",
       "      <th>text</th>\n",
       "      <th>claim_amount</th>\n",
       "      <th>claim_status</th>\n",
       "      <th>communication_history</th>\n",
       "      <th>encoded_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21753</th>\n",
       "      <td>property</td>\n",
       "      <td>703742</td>\n",
       "      <td>Lucas Hall</td>\n",
       "      <td>2022-09-12</td>\n",
       "      <td>3633</td>\n",
       "      <td>There wa a significant incident on 2022-09-12 ...</td>\n",
       "      <td>64855</td>\n",
       "      <td>Approved</td>\n",
       "      <td>I request an appraisal of the property damage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>auto</td>\n",
       "      <td>521914</td>\n",
       "      <td>Isabella Jones</td>\n",
       "      <td>2023-06-07</td>\n",
       "      <td>1700</td>\n",
       "      <td>On 2023-06-07 , Vehicle Z encountered a vehicl...</td>\n",
       "      <td>49166</td>\n",
       "      <td>Pending</td>\n",
       "      <td>I request an analysis of the impact of the aut...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22941</th>\n",
       "      <td>property</td>\n",
       "      <td>976754</td>\n",
       "      <td>James Martin</td>\n",
       "      <td>2023-03-12</td>\n",
       "      <td>4538</td>\n",
       "      <td>On 2023-03-12 , a critical [ Property Incident...</td>\n",
       "      <td>69885</td>\n",
       "      <td>Pending</td>\n",
       "      <td>I am writing to report a property damage incid...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>health</td>\n",
       "      <td>341419</td>\n",
       "      <td>Jack Smith</td>\n",
       "      <td>2022-07-06</td>\n",
       "      <td>8763</td>\n",
       "      <td>An alarming incident involving Jack Smith occu...</td>\n",
       "      <td>46590</td>\n",
       "      <td>Denied</td>\n",
       "      <td>I request an analysis of the impact of the hea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17090</th>\n",
       "      <td>health</td>\n",
       "      <td>700105</td>\n",
       "      <td>Henry Taylor</td>\n",
       "      <td>2023-02-21</td>\n",
       "      <td>6461</td>\n",
       "      <td>An incident occurred on 2023-02-21 where Henry...</td>\n",
       "      <td>2995</td>\n",
       "      <td>Approved</td>\n",
       "      <td>We have reviewed the detail of the health inci...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29802</th>\n",
       "      <td>property</td>\n",
       "      <td>364771</td>\n",
       "      <td>Amelia Davis</td>\n",
       "      <td>2023-04-29</td>\n",
       "      <td>6874</td>\n",
       "      <td>There wa a major [ Property Incident 32 ] on 2...</td>\n",
       "      <td>90131</td>\n",
       "      <td>Denied</td>\n",
       "      <td>Please inform me of any deductible amount or e...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>property</td>\n",
       "      <td>574563</td>\n",
       "      <td>Benjamin Wilson</td>\n",
       "      <td>2022-10-02</td>\n",
       "      <td>3287</td>\n",
       "      <td>A distressing [ Property Incident 34 ] took pl...</td>\n",
       "      <td>10579</td>\n",
       "      <td>Pending</td>\n",
       "      <td>I would like to discus the possibility of an i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>auto</td>\n",
       "      <td>702699</td>\n",
       "      <td>Lucas Walker</td>\n",
       "      <td>2023-04-06</td>\n",
       "      <td>8053</td>\n",
       "      <td>A vandalism incident took place on 2023-04-06 ...</td>\n",
       "      <td>78754</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>Please share any recommendation or suggestion ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>health</td>\n",
       "      <td>165495</td>\n",
       "      <td>William Smith</td>\n",
       "      <td>2023-06-18</td>\n",
       "      <td>1267</td>\n",
       "      <td>On 2023-06-18 , a broken arm occurred to Willi...</td>\n",
       "      <td>88911</td>\n",
       "      <td>Pending</td>\n",
       "      <td>I would appreciate a summary of the health inc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23654</th>\n",
       "      <td>health</td>\n",
       "      <td>866241</td>\n",
       "      <td>Benjamin Anderson</td>\n",
       "      <td>2022-12-10</td>\n",
       "      <td>6021</td>\n",
       "      <td>On 2022-12-10 , a severe fracture occurred to ...</td>\n",
       "      <td>72634</td>\n",
       "      <td>Approved</td>\n",
       "      <td>Please provide additional information regardin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24000 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          label  policy_num       patient_name        date  policy_claim_no  \\\n",
       "21753  property      703742         Lucas Hall  2022-09-12             3633   \n",
       "251        auto      521914     Isabella Jones  2023-06-07             1700   \n",
       "22941  property      976754       James Martin  2023-03-12             4538   \n",
       "618      health      341419         Jack Smith  2022-07-06             8763   \n",
       "17090    health      700105       Henry Taylor  2023-02-21             6461   \n",
       "...         ...         ...                ...         ...              ...   \n",
       "29802  property      364771       Amelia Davis  2023-04-29             6874   \n",
       "5390   property      574563    Benjamin Wilson  2022-10-02             3287   \n",
       "860        auto      702699       Lucas Walker  2023-04-06             8053   \n",
       "15795    health      165495      William Smith  2023-06-18             1267   \n",
       "23654    health      866241  Benjamin Anderson  2022-12-10             6021   \n",
       "\n",
       "                                                    text  claim_amount  \\\n",
       "21753  There wa a significant incident on 2022-09-12 ...         64855   \n",
       "251    On 2023-06-07 , Vehicle Z encountered a vehicl...         49166   \n",
       "22941  On 2023-03-12 , a critical [ Property Incident...         69885   \n",
       "618    An alarming incident involving Jack Smith occu...         46590   \n",
       "17090  An incident occurred on 2023-02-21 where Henry...          2995   \n",
       "...                                                  ...           ...   \n",
       "29802  There wa a major [ Property Incident 32 ] on 2...         90131   \n",
       "5390   A distressing [ Property Incident 34 ] took pl...         10579   \n",
       "860    A vandalism incident took place on 2023-04-06 ...         78754   \n",
       "15795  On 2023-06-18 , a broken arm occurred to Willi...         88911   \n",
       "23654  On 2022-12-10 , a severe fracture occurred to ...         72634   \n",
       "\n",
       "      claim_status                              communication_history  \\\n",
       "21753     Approved  I request an appraisal of the property damage ...   \n",
       "251        Pending  I request an analysis of the impact of the aut...   \n",
       "22941      Pending  I am writing to report a property damage incid...   \n",
       "618         Denied  I request an analysis of the impact of the hea...   \n",
       "17090     Approved  We have reviewed the detail of the health inci...   \n",
       "...            ...                                                ...   \n",
       "29802       Denied  Please inform me of any deductible amount or e...   \n",
       "5390       Pending  I would like to discus the possibility of an i...   \n",
       "860    In Progress  Please share any recommendation or suggestion ...   \n",
       "15795      Pending  I would appreciate a summary of the health inc...   \n",
       "23654     Approved  Please provide additional information regardin...   \n",
       "\n",
       "       encoded_label  \n",
       "21753              2  \n",
       "251                0  \n",
       "22941              2  \n",
       "618                1  \n",
       "17090              1  \n",
       "...              ...  \n",
       "29802              2  \n",
       "5390               2  \n",
       "860                0  \n",
       "15795              1  \n",
       "23654              1  \n",
       "\n",
       "[24000 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>policy_num</th>\n",
       "      <th>patient_name</th>\n",
       "      <th>date</th>\n",
       "      <th>policy_claim_no</th>\n",
       "      <th>text</th>\n",
       "      <th>claim_amount</th>\n",
       "      <th>claim_status</th>\n",
       "      <th>communication_history</th>\n",
       "      <th>encoded_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>health</td>\n",
       "      <td>539372</td>\n",
       "      <td>Daniel Wilson</td>\n",
       "      <td>2023-03-19</td>\n",
       "      <td>6061</td>\n",
       "      <td>There wa a severe asthma attack on 2023-03-19 ...</td>\n",
       "      <td>56293</td>\n",
       "      <td>Approved</td>\n",
       "      <td>I would like to analyze the root cause of the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22404</th>\n",
       "      <td>health</td>\n",
       "      <td>933642</td>\n",
       "      <td>Sebastian Taylor</td>\n",
       "      <td>2022-08-21</td>\n",
       "      <td>1597</td>\n",
       "      <td>An incident occurred on 2022-08-21 where Sebas...</td>\n",
       "      <td>60699</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>Please share any witness statement or testimon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23397</th>\n",
       "      <td>health</td>\n",
       "      <td>457744</td>\n",
       "      <td>Emma Miller</td>\n",
       "      <td>2023-01-11</td>\n",
       "      <td>5072</td>\n",
       "      <td>An incident occurred on 2023-01-11 where Emma ...</td>\n",
       "      <td>70258</td>\n",
       "      <td>Approved</td>\n",
       "      <td>I request an analysis of the impact of the hea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25058</th>\n",
       "      <td>auto</td>\n",
       "      <td>540601</td>\n",
       "      <td>Alexander Lee</td>\n",
       "      <td>2023-04-13</td>\n",
       "      <td>9898</td>\n",
       "      <td>There wa an incident on 2023-04-13 where Vehic...</td>\n",
       "      <td>73800</td>\n",
       "      <td>Denied</td>\n",
       "      <td>Please submit all relevant documentation relat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2664</th>\n",
       "      <td>auto</td>\n",
       "      <td>193591</td>\n",
       "      <td>Mia Davis</td>\n",
       "      <td>2022-11-13</td>\n",
       "      <td>1220</td>\n",
       "      <td>A significant car accident took place on 2022-...</td>\n",
       "      <td>40412</td>\n",
       "      <td>Approved</td>\n",
       "      <td>We need to ensure proper documentation and rec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2210</th>\n",
       "      <td>health</td>\n",
       "      <td>569687</td>\n",
       "      <td>Elijah Moore</td>\n",
       "      <td>2022-10-19</td>\n",
       "      <td>4650</td>\n",
       "      <td>There wa a critical internal injury incident o...</td>\n",
       "      <td>34988</td>\n",
       "      <td>Pending</td>\n",
       "      <td>Please provide a timeline of event leading up ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14144</th>\n",
       "      <td>property</td>\n",
       "      <td>146599</td>\n",
       "      <td>Sebastian Williams</td>\n",
       "      <td>2023-05-19</td>\n",
       "      <td>5823</td>\n",
       "      <td>A severe incident took place on 2023-05-19 at ...</td>\n",
       "      <td>85050</td>\n",
       "      <td>Denied</td>\n",
       "      <td>Please inform me of any deductible amount or e...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23108</th>\n",
       "      <td>auto</td>\n",
       "      <td>102360</td>\n",
       "      <td>James Martin</td>\n",
       "      <td>2022-08-11</td>\n",
       "      <td>7330</td>\n",
       "      <td>On 2022-08-11 , a minor collision occurred inv...</td>\n",
       "      <td>24668</td>\n",
       "      <td>Denied</td>\n",
       "      <td>I request a meeting to discus the auto inciden...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25703</th>\n",
       "      <td>auto</td>\n",
       "      <td>414942</td>\n",
       "      <td>Sophia Hall</td>\n",
       "      <td>2022-09-18</td>\n",
       "      <td>6922</td>\n",
       "      <td>There wa a major automobile collision on 2022-...</td>\n",
       "      <td>11596</td>\n",
       "      <td>Denied</td>\n",
       "      <td>We need to investigate the circumstance surrou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29171</th>\n",
       "      <td>health</td>\n",
       "      <td>364172</td>\n",
       "      <td>Amelia Thomas</td>\n",
       "      <td>2023-04-22</td>\n",
       "      <td>3438</td>\n",
       "      <td>On 2023-04-22 , Amelia Thomas encountered a di...</td>\n",
       "      <td>44045</td>\n",
       "      <td>Denied</td>\n",
       "      <td>We have reviewed the detail of the health inci...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          label  policy_num        patient_name        date  policy_claim_no  \\\n",
       "2308     health      539372       Daniel Wilson  2023-03-19             6061   \n",
       "22404    health      933642    Sebastian Taylor  2022-08-21             1597   \n",
       "23397    health      457744         Emma Miller  2023-01-11             5072   \n",
       "25058      auto      540601       Alexander Lee  2023-04-13             9898   \n",
       "2664       auto      193591           Mia Davis  2022-11-13             1220   \n",
       "...         ...         ...                 ...         ...              ...   \n",
       "2210     health      569687        Elijah Moore  2022-10-19             4650   \n",
       "14144  property      146599  Sebastian Williams  2023-05-19             5823   \n",
       "23108      auto      102360        James Martin  2022-08-11             7330   \n",
       "25703      auto      414942         Sophia Hall  2022-09-18             6922   \n",
       "29171    health      364172       Amelia Thomas  2023-04-22             3438   \n",
       "\n",
       "                                                    text  claim_amount  \\\n",
       "2308   There wa a severe asthma attack on 2023-03-19 ...         56293   \n",
       "22404  An incident occurred on 2022-08-21 where Sebas...         60699   \n",
       "23397  An incident occurred on 2023-01-11 where Emma ...         70258   \n",
       "25058  There wa an incident on 2023-04-13 where Vehic...         73800   \n",
       "2664   A significant car accident took place on 2022-...         40412   \n",
       "...                                                  ...           ...   \n",
       "2210   There wa a critical internal injury incident o...         34988   \n",
       "14144  A severe incident took place on 2023-05-19 at ...         85050   \n",
       "23108  On 2022-08-11 , a minor collision occurred inv...         24668   \n",
       "25703  There wa a major automobile collision on 2022-...         11596   \n",
       "29171  On 2023-04-22 , Amelia Thomas encountered a di...         44045   \n",
       "\n",
       "      claim_status                              communication_history  \\\n",
       "2308      Approved  I would like to analyze the root cause of the ...   \n",
       "22404  In Progress  Please share any witness statement or testimon...   \n",
       "23397     Approved  I request an analysis of the impact of the hea...   \n",
       "25058       Denied  Please submit all relevant documentation relat...   \n",
       "2664      Approved  We need to ensure proper documentation and rec...   \n",
       "...            ...                                                ...   \n",
       "2210       Pending  Please provide a timeline of event leading up ...   \n",
       "14144       Denied  Please inform me of any deductible amount or e...   \n",
       "23108       Denied  I request a meeting to discus the auto inciden...   \n",
       "25703       Denied  We need to investigate the circumstance surrou...   \n",
       "29171       Denied  We have reviewed the detail of the health inci...   \n",
       "\n",
       "       encoded_label  \n",
       "2308               1  \n",
       "22404              1  \n",
       "23397              1  \n",
       "25058              0  \n",
       "2664               0  \n",
       "...              ...  \n",
       "2210               1  \n",
       "14144              2  \n",
       "23108              0  \n",
       "25703              0  \n",
       "29171              1  \n",
       "\n",
       "[6000 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df = pd.read_csv('insurance_synthetic_data.csv')\n",
    "df  = pd.read_csv('insurance_claims.csv')\n",
    "df = df.rename(columns = {'incident_class':'label'})\n",
    "df = df.rename(columns = {'incident_description':'text'})\n",
    "# display(df)\n",
    "print(df.label.unique())\n",
    "\n",
    "\n",
    "# Divide the data into train, validation, and test sets\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "# val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Print the sizes of the datasets\n",
    "print(\"Train set size:\", len(train))\n",
    "print(\"Test set size:\", len(test))\n",
    "\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "labelencoder.fit(train['label'])\n",
    "train['encoded_label'] = labelencoder.fit_transform(train['label'])\n",
    "test[\"encoded_label\"] = labelencoder.transform(test['label'])\n",
    "display(train)\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa402815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train encodings shape  torch.Size([24000, 12])\n",
      "test encodings shape  torch.Size([6000, 12])\n"
     ]
    }
   ],
   "source": [
    "train_labels = torch.tensor(train[\"encoded_label\"].tolist())\n",
    "test_labels = torch.tensor(test[\"encoded_label\"].tolist())\n",
    "\n",
    "# Convert your train text data to a list\n",
    "train_texts = train[\"text\"].tolist()\n",
    "test_texts = test[\"text\"].tolist()\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "\n",
    "train_encodings = tokenizer(\n",
    "    train_texts,\n",
    "    padding=True,           # pad all inputs to max length\n",
    "    max_length=12,         # Bert max is 512, we choose 24 for computational efficiency\n",
    "    return_tensors=\"pt\",    # Return format pytorch tensor\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "print('train encodings shape ', train_encodings['input_ids'].shape)\n",
    "\n",
    "\n",
    "test_encodings = tokenizer(\n",
    "    test_texts,\n",
    "    padding=True,           # pad all inputs to max length\n",
    "    max_length=12,         # Bert max is 512, we choose 24 for computational efficiency\n",
    "    return_tensors=\"pt\",    # Return format pytorch tensor\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "print('test encodings shape ', test_encodings['input_ids'].shape)\n",
    "\n",
    "\n",
    "# Convert the tokenized inputs into a PyTorch dataset\n",
    "train_ds = TensorDataset(\n",
    "    train_encodings[\"input_ids\"],\n",
    "    train_encodings[\"attention_mask\"],\n",
    "    train_labels  # Assuming you have the corresponding train labels\n",
    ")\n",
    "\n",
    "\n",
    "# Convert the tokenized inputs into a PyTorch dataset\n",
    "test_ds = TensorDataset(\n",
    "    test_encodings[\"input_ids\"],\n",
    "    test_encodings[\"attention_mask\"],\n",
    "    test_labels  # Assuming you have the corresponding train labels\n",
    ")\n",
    "\n",
    "\n",
    "# Define batch size for the train_loader and test_loader\n",
    "batch_size = 16\n",
    "\n",
    "# Create the train_loader\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Create the test_loader\n",
    "# for i in test_loader:\n",
    "#     print(i)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50777dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f11574408e4a17aebbbfe3b5c08b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Average Loss: 0.1150\n",
      "Epoch 2/3, Average Loss: 0.0794\n",
      "Epoch 3/3, Average Loss: 0.0794\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "base_model1 = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "\n",
    "\n",
    "\n",
    "# Set the device for training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model1 = base_model1.to(device)\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model1.parameters(), lr=5e-5)\n",
    "\n",
    "# Set the number of epochs and calculate the total training steps\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "\n",
    "# Training loop\n",
    "model1.train()\n",
    "progress_bar = tqdm(total=num_training_steps)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    epoch_loss = 0.0 \n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model1(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()  # Accumulate the loss\n",
    "#         print('Loss item', loss.item())\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "    avg_loss = epoch_loss / len(train_loader)  # Calculate average loss for the epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb6928f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.93      1970\n",
      "           1       1.00      0.86      0.93      2026\n",
      "           2       1.00      0.98      0.99      2004\n",
      "\n",
      "    accuracy                           0.95      6000\n",
      "   macro avg       0.95      0.95      0.95      6000\n",
      "weighted avg       0.95      0.95      0.95      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluation loop\n",
    "model1.eval()\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Disable gradient calculation\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model1(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Get predicted labels\n",
    "#         _, predicted = torch.max(logits, dim=1)\n",
    "        predicted = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        predictions.extend(predicted.cpu().tolist())\n",
    "        true_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "# Calculate classification report\n",
    "classification_rep = classification_report(true_labels, predictions)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f81f2e2",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "https://huggingface.co/docs/transformers/training\n",
    "\n",
    "https://mccormickml.com/2019/07/22/BERT-fine-tuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84be3394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c359f1fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c190bab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6863163e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616495e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86c6d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb294c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03abf38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a8f511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3ba2a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8251529f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c4309b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687acdb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf16f30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef07c7f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454f4822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b9188c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6ff153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insurance_claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17ec608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path=\"/home/nitesh/Documents/MY_THESIS/MAMI\"\n",
    "\n",
    "# train_filename=\"training1.csv\"\n",
    "# train_data_file=os.path.join(file_path, train_filename)\n",
    "# train2 = pd.read_csv(train_data_file, delimiter='\\t')\n",
    "\n",
    "\n",
    "\n",
    "# IDs = [] \n",
    "# images = []\n",
    "# directory = os.path.join(file_path, 'TRAINING')   # directory where we have images \n",
    "# filenames = natsort.natsorted(os.listdir(directory))  \n",
    "\n",
    "# # get the ids from the images, where images are having three channels; omit images if channels != 3\n",
    "# for i, filename in enumerate(filenames):\n",
    "# #     print(i, filename)\n",
    "#     if filename.endswith(\".jpg\"):\n",
    "# #         ID = int(filename[:-4])\n",
    "#         ID = filename\n",
    "#         pathname = os.path.join(directory, filename)\n",
    "#         im = Image.open(pathname)\n",
    "#         im = im.resize((224, 224))  # Resize the image to (224, 224)\n",
    "#         imnp = np.array(im)\n",
    "#         if len(imnp.shape) != 3:\n",
    "# #             print(\"This is 1 channel, so we omit it\", imnp.shape, filename)\n",
    "#             continue\n",
    "#         IDs.append(ID)\n",
    "#         images.append(imnp)\n",
    "\n",
    "# def get_common_strings(list1, list2):\n",
    "#     return list(set(list1) & set(list2))\n",
    "\n",
    "# # Example usage\n",
    "# list1 = IDs\n",
    "# list2 = list(train2.file_name)  #from the text file where we have text description \n",
    "# common_strings = get_common_strings(list1, list2)\n",
    "# print('len of common strings', len(common_strings))\n",
    "\n",
    "# sorted_ids = natsort.natsorted(common_strings)\n",
    "\n",
    "# # print(sorted_ids)\n",
    "\n",
    "# # Sort the dataframe with natural ordering of the IDs\n",
    "# train2['prefix_file_name'] = train2['file_name'].str.extract('(\\d+)').astype(int)\n",
    "# # Assuming 'df' is your DataFrame\n",
    "# sorted_train_df = train2.sort_values(by='prefix_file_name', ascending=True)\n",
    "# sorted_train_df\n",
    "\n",
    "# # Assuming 'df' is your DataFrame and 'common_strings' is the list of strings\n",
    "# # Get the common string values in the column \n",
    "# filtered_df = sorted_train_df[sorted_train_df['file_name'].isin(sorted_ids)].reset_index(drop=True)\n",
    "\n",
    "# # Print the filtered DataFrame\n",
    "# print('filtered_df shape', filtered_df.shape)\n",
    "\n",
    "# train3 = filtered_df.copy()\n",
    "# # print(train3.shape)\n",
    "\n",
    "# trainx = train3.rename(columns={'Text Transcription': 'text'})\n",
    "# # display(trainx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ### LOAD TEST DATA\n",
    "# test_filename=\"Test.csv\"\n",
    "# test_data_file=os.path.join(file_path, test_filename)\n",
    "# test1 = pd.read_csv(test_data_file, delimiter='\\t')\n",
    "\n",
    "\n",
    "# test_labels_filename = 'test_labels.txt'\n",
    "# test_lbls_file = os.path.join(file_path, test_labels_filename)\n",
    "# test_labels = pd.read_csv(test_lbls_file, \n",
    "#                           delimiter='\\t',\n",
    "#                           header=None)\n",
    "\n",
    "# test_labels.columns = ['file_name', \n",
    "#                       \"misogynous\",\n",
    "#                        \"shaming\",\n",
    "#                        \"stereotype\",\n",
    "#                        \"objectification\",\n",
    "#                        \"violence\"]\n",
    "\n",
    "# merged_test = pd.merge(test1, test_labels, on='file_name', how='inner')\n",
    "\n",
    "\n",
    "# # Sort the dataframe with natural ordering of the IDs\n",
    "# merged_test['prefix_file_name'] = merged_test['file_name'].str.extract('(\\d+)').astype(int)\n",
    "# # Assuming 'df' is your DataFrame\n",
    "# merged_test1 = merged_test.sort_values(by='prefix_file_name', ascending=True)\n",
    "# merged_test1\n",
    "\n",
    "\n",
    "# # # train = train.rename(columns={'Text Transcription': 'text'})\n",
    "# test2 = merged_test1.rename(columns={'Text Transcription': 'text'})\n",
    "# # test2\n",
    "\n",
    "\n",
    "\n",
    "# # ######################################################\n",
    "# train = trainx[['file_name', 'text', 'misogynous']]\n",
    "# train = train.rename(columns = {'misogynous':'label'})\n",
    "\n",
    "# test = test2[['file_name', 'text', 'misogynous']]\n",
    "# test = test.rename(columns = {'misogynous':'label'})\n",
    "\n",
    "# # train['label'] = train['label'].map({0: 'non_misogyn', 1: 'misogyn'})\n",
    "# # test['label'] = test['label'].map({0: 'non_misogyn', 1: 'misogyn'})\n",
    "# display(train)\n",
    "# display(test)\n",
    "\n",
    "\n",
    "# # import pandas as pd\n",
    "\n",
    "# # # Assuming you have two dataframes named train_df and test_df\n",
    "# # combined_df = pd.concat([train, test], ignore_index=True)\n",
    "# # combined_df = combined_df[['text', 'label']]\n",
    "# # display(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac179963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the CSV file\n",
    "# df = pd.read_csv('insurance_synthetic_data.csv')\n",
    "\n",
    "# # Divide the data into train, validation, and test sets\n",
    "# train1, test1 = train_test_split(combined_df, test_size=0.2, random_state=42)\n",
    "# # val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# # Print the sizes of the datasets\n",
    "# print(\"Train set size:\", len(train1))\n",
    "# # print(\"Validation set size:\", len(val_df))\n",
    "# print(\"Test set size:\", len(test1))\n",
    "\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "labelencoder.fit(train['label'])\n",
    "train['encoded_label'] = labelencoder.fit_transform(train['label'])\n",
    "test[\"encoded_label\"] = labelencoder.transform(test['label'])\n",
    "display(train)\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb5a326",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.tensor(train[\"encoded_label\"].tolist())\n",
    "test_labels = torch.tensor(test[\"encoded_label\"].tolist())\n",
    "\n",
    "# Convert your train text data to a list\n",
    "train_texts = train[\"text\"].tolist()\n",
    "test_texts = test[\"text\"].tolist()\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "\n",
    "train_encodings = tokenizer(\n",
    "    train_texts,\n",
    "    padding=True,           # pad all inputs to max length\n",
    "    max_length=12,         # Bert max is 512, we choose 24 for computational efficiency\n",
    "    return_tensors=\"pt\",    # Return format pytorch tensor\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "print('train encodings shape ', train_encodings['input_ids'].shape)\n",
    "\n",
    "\n",
    "test_encodings = tokenizer(\n",
    "    test_texts,\n",
    "    padding=True,           # pad all inputs to max length\n",
    "    max_length=12,         # Bert max is 512, we choose 24 for computational efficiency\n",
    "    return_tensors=\"pt\",    # Return format pytorch tensor\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "print('test encodings shape ', test_encodings['input_ids'].shape)\n",
    "\n",
    "\n",
    "# Convert the tokenized inputs into a PyTorch dataset\n",
    "train_ds = TensorDataset(\n",
    "    train_encodings[\"input_ids\"],\n",
    "    train_encodings[\"attention_mask\"],\n",
    "    train_labels  # Assuming you have the corresponding train labels\n",
    ")\n",
    "\n",
    "\n",
    "# Convert the tokenized inputs into a PyTorch dataset\n",
    "test_ds = TensorDataset(\n",
    "    test_encodings[\"input_ids\"],\n",
    "    test_encodings[\"attention_mask\"],\n",
    "    test_labels  # Assuming you have the corresponding train labels\n",
    ")\n",
    "\n",
    "\n",
    "# Define batch size for the train_loader and test_loader\n",
    "batch_size = 16\n",
    "\n",
    "# Create the train_loader\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Create the test_loader\n",
    "# for i in test_loader:\n",
    "#     print(i)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4776c40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "base_model1 = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "\n",
    "\n",
    "# Set the device for training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model1 = base_model1.to(device)\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model1.parameters(), lr=5e-5)\n",
    "\n",
    "# Set the number of epochs and calculate the total training steps\n",
    "num_epochs = 8\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "\n",
    "# Training loop\n",
    "model1.train()\n",
    "progress_bar = tqdm(total=num_training_steps)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    epoch_loss = 0.0 \n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model1(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()  # Accumulate the loss\n",
    "#         print('Loss item', loss.item())\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "    avg_loss = epoch_loss / len(train_loader)  # Calculate average loss for the epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda65c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluation loop\n",
    "model1.eval()\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Disable gradient calculation\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model1(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Get predicted labels\n",
    "#         _, predicted = torch.max(logits, dim=1)\n",
    "        predicted = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        predictions.extend(predicted.cpu().tolist())\n",
    "        true_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "# Calculate classification report\n",
    "classification_rep = classification_report(true_labels, predictions)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef696fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c8720f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fad371a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97d83d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4493f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e325757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3b7138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fabe8c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa73b531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c03186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa54f54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f4908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dbd2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0bf1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3162ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('insurance_synthetic_data.csv')\n",
    "df  = pd.read_csv('insurance_claims.csv')\n",
    "df = df.rename(columns = {'incident_class':'label'})\n",
    "df = df.rename(columns = {'incident_description':'text'})\n",
    "display(df)\n",
    "print(df.label.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8395d485",
   "metadata": {},
   "source": [
    "## Train Test Split and Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20904133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the CSV file\n",
    "# df = pd.read_csv('insurance_synthetic_data.csv')\n",
    "\n",
    "# Divide the data into train, validation, and test sets\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "# val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Print the sizes of the datasets\n",
    "print(\"Train set size:\", len(train))\n",
    "# print(\"Validation set size:\", len(val_df))\n",
    "print(\"Test set size:\", len(test))\n",
    "\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "labelencoder.fit(train['label'])\n",
    "train['encoded_label'] = labelencoder.fit_transform(train['label'])\n",
    "test[\"encoded_label\"] = labelencoder.transform(test['label'])\n",
    "display(train)\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bf7ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.tensor(train[\"encoded_label\"].tolist())\n",
    "test_labels = torch.tensor(test[\"encoded_label\"].tolist())\n",
    "\n",
    "# Convert your train text data to a list\n",
    "train_texts = train[\"text\"].tolist()\n",
    "test_texts = test[\"text\"].tolist()\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "\n",
    "train_encodings = tokenizer(\n",
    "    train_texts,\n",
    "    padding=True,           # pad all inputs to max length\n",
    "    max_length=20,         # Bert max is 512, we choose 24 for computational efficiency\n",
    "    return_tensors=\"pt\",    # Return format pytorch tensor\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "print('train encodings shape ', train_encodings['input_ids'].shape)\n",
    "\n",
    "\n",
    "test_encodings = tokenizer(\n",
    "    test_texts,\n",
    "    padding=True,           # pad all inputs to max length\n",
    "    max_length=20,         # Bert max is 512, we choose 24 for computational efficiency\n",
    "    return_tensors=\"pt\",    # Return format pytorch tensor\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "print('test encodings shape ', test_encodings['input_ids'].shape)\n",
    "\n",
    "\n",
    "# Convert the tokenized inputs into a PyTorch dataset\n",
    "train_ds = TensorDataset(\n",
    "    train_encodings[\"input_ids\"],\n",
    "    train_encodings[\"attention_mask\"],\n",
    "    train_labels  # Assuming you have the corresponding train labels\n",
    ")\n",
    "\n",
    "\n",
    "# Convert the tokenized inputs into a PyTorch dataset\n",
    "test_ds = TensorDataset(\n",
    "    test_encodings[\"input_ids\"],\n",
    "    test_encodings[\"attention_mask\"],\n",
    "    test_labels  # Assuming you have the corresponding train labels\n",
    ")\n",
    "\n",
    "\n",
    "# Define batch size for the train_loader and test_loader\n",
    "batch_size = 16\n",
    "\n",
    "# Create the train_loader\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Create the test_loader\n",
    "# for i in test_loader:\n",
    "#     print(i)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "base_model1 = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "\n",
    "\n",
    "\n",
    "# Set the device for training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model1 = base_model1.to(device)\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model1.parameters(), lr=5e-5)\n",
    "\n",
    "# Set the number of epochs and calculate the total training steps\n",
    "num_epochs = 5\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "\n",
    "# Training loop\n",
    "model1.train()\n",
    "progress_bar = tqdm(total=num_training_steps)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    epoch_loss = 0.0 \n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model1(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()  # Accumulate the loss\n",
    "#         print('Loss item', loss.item())\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "    avg_loss = epoch_loss / len(train_loader)  # Calculate average loss for the epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d95b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluation loop\n",
    "model1.eval()\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Disable gradient calculation\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model1(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Get predicted labels\n",
    "#         _, predicted = torch.max(logits, dim=1)\n",
    "        predicted = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        predictions.extend(predicted.cpu().tolist())\n",
    "        true_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "# Calculate classification report\n",
    "classification_rep = classification_report(true_labels, predictions)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921eeeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "# Evaluation loop\n",
    "model1.eval()\n",
    "\n",
    "# predictions = []\n",
    "# true_labels = []\n",
    "\n",
    "# Disable gradient calculation\n",
    "# with torch.no_grad():\n",
    "for batch in test_loader:\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    labels = labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outputs = model1(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    predicted = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predicted, references=labels)\n",
    "\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778b5981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a486fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a579d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726d13bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf1efcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458439f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be16c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1a082c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e9b4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb7993e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f109e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d474b971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960c668d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7cc202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70035e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import BertModel\n",
    "\n",
    "# # Load pre-trained BERT model\n",
    "# base_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fd5031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import BertModel\n",
    "\n",
    "# # Load pre-trained BERT model\n",
    "# base_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Freeze all the base model's parameters, including embeddings\n",
    "# for param in base_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "    \n",
    "\n",
    "# # Define your classification layer\n",
    "# num_classes = 2\n",
    "# classification_layer = nn.Linear(base_model.config.hidden_size, num_classes)\n",
    "\n",
    "\n",
    "# # Create a new model with the classification layer on top of the base model\n",
    "# class ModelWithClassifier(nn.Module):\n",
    "#     def __init__(self, base_model, classification_layer):\n",
    "#         super(ModelWithClassifier, self).__init__()\n",
    "#         self.base_model = base_model\n",
    "#         self.classification_layer = classification_layer\n",
    "\n",
    "#     def forward(self, inputs, attention_mask):\n",
    "#         outputs = self.base_model(inputs, attention_mask=attention_mask)\n",
    "# #         pooled_output = outputs.pooler_output  # Use pooled_output for classification\n",
    "# #         logits = self.classification_layer(pooled_output)\n",
    "#         last_hidden_state = outputs.last_hidden_state\n",
    "#         logits = self.classification_layer(last_hidden_state[:, 0, :])  # Select first token [CLS]\n",
    "#         return logits\n",
    "\n",
    "# model = ModelWithClassifier(base_model, classification_layer)\n",
    "\n",
    "# # Set the device for training\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Define your loss function and optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(classification_layer.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98b7cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import torch\n",
    "# # import numpy as np\n",
    "\n",
    "# # # Set the random seed for PyTorch\n",
    "# # torch.manual_seed(42)\n",
    "# # torch.backends.cudnn.deterministic = True\n",
    "# # torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# # # Set the random seed for NumPy\n",
    "# # np.random.seed(42)\n",
    "\n",
    "# for inputs, attention_masks, labels in train_loader:\n",
    "#     inputs = inputs.to(device)\n",
    "#     attention_masks = attention_masks.to(device)\n",
    "#     labels = labels.to(device)\n",
    "\n",
    "#     # Forward pass\n",
    "# #     logits = model(inputs, attention_mask=attention_masks)\n",
    "#     logits = model.forward(inputs, attention_mask=attention_masks)\n",
    "#     print(logits)\n",
    "\n",
    "#     # Compute the loss and perform backpropagation\n",
    "#     loss = criterion(logits, labels)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8157890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()  # Set the model to evaluation mode\n",
    "# total_correct = 0\n",
    "# total_samples = 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for inputs, attention_masks, labels in test_loader:\n",
    "#         inputs = inputs.to(device)\n",
    "#         attention_masks = attention_masks.to(device)\n",
    "#         labels = labels.to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         logits = model.forward(inputs, attention_mask=attention_masks)\n",
    "#         _, predictions = torch.max(logits, dim=1)\n",
    "#         print(\"Predictions:\", predictions)\n",
    "#         print('Labels:', labels)\n",
    "\n",
    "\n",
    "# #         # Update evaluation metrics\n",
    "# #         total_correct += (predictions == labels).sum().item()\n",
    "# #         total_samples += labels.size(0)\n",
    "\n",
    "# # accuracy = total_correct / total_samples\n",
    "# # print(\"Test Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f71f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec866c92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cc2d749",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/training\n",
    "https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
    "\n",
    "https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Linear.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
