{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fff3601",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6decb56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 15:19:21.750792: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/nitesh/env/dev38/python38/lib/python3.8/site-packages/xgboost/compat.py:93: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import BertModel\n",
    "from transformers import AutoTokenizer,AutoModel\n",
    "from torch.utils.data import Dataset\n",
    "# from transformers import AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "print(transformers.__version__)\n",
    "# !pip install evaluate\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446edbf8",
   "metadata": {},
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2972d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_classification_report(report, mod, exec_time, eval_set=None):\n",
    "    lines = report.split('\\n')\n",
    "    data = lines[2:4] + lines[5:9]\n",
    "    data = [line.split() for line in data]\n",
    "    \n",
    "\n",
    "    acc = float(data[3][1])\n",
    "    m_prec = float(data[4][2])\n",
    "    m_recall = float(data[4][3])\n",
    "    m_f1 = float(data[4][4])\n",
    "    w_prec = float(data[5][2])\n",
    "    w_recall = float(data[5][3])\n",
    "    w_f1 = float(data[5][4])\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'Model': [mod],\n",
    "        'EvalSet': [eval_set],\n",
    "        'Accuracy': [acc],\n",
    "        'M-Precision': [m_prec],\n",
    "        'M-Recall': [m_recall],\n",
    "        'M-F1-Score': [m_f1],\n",
    "        'W-Precision': [w_prec],\n",
    "        'W-Recall': [w_recall],\n",
    "        'W-F1-Score': [w_f1],\n",
    "        'Runtime': [exec_time]\n",
    "    })\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc42a123",
   "metadata": {},
   "source": [
    "## 1.  Data Loading\n",
    "## 2. Data Preprocessing\n",
    "## 3. Generate Pretrained BERT Embeddings\n",
    "## 4. Model Training\n",
    "## 5. Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80238235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim Categories: ['property' 'auto' 'health']\n",
      "Training dataset size: 24000\n",
      "Test dataset size: 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20126/3419089972.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[\"encoded_label\"] = labelencoder.fit_transform(train['label'])\n",
      "/tmp/ipykernel_20126/3419089972.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test[\"encoded_label\"] = labelencoder.transform(test['label'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Device: NVIDIA GeForce RTX 3070 Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/tmp/ipykernel_20126/3419089972.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_tokens = {key: torch.tensor(val).to(device) for key, val in batch_tokens.items()}\n",
      "/tmp/ipykernel_20126/3419089972.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_tokens = {key: torch.tensor(val).to(device) for key, val in batch_tokens.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape:  torch.Size([19200, 768]) validation data shape:  torch.Size([4800, 768]) test embd shape: torch.Size([6000, 768])\n",
      "Epoch 1/5, Train Loss: 0.1877\n",
      "Epoch 1/5, Validation Loss: 0.0941\n",
      "Epoch 2/5, Train Loss: 0.0888\n",
      "Epoch 2/5, Validation Loss: 0.0834\n",
      "Epoch 3/5, Train Loss: 0.0818\n",
      "Epoch 3/5, Validation Loss: 0.0824\n",
      "Epoch 4/5, Train Loss: 0.0832\n",
      "Epoch 4/5, Validation Loss: 0.0834\n",
      "Epoch 5/5, Train Loss: 0.0796\n",
      "Epoch 5/5, Validation Loss: 0.0826\n",
      "Execution time: 29.679775953292847 seconds\n",
      "Training Data Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.942     0.923     0.932      6484\n",
      "           1      0.924     0.965     0.944      6359\n",
      "           2      1.000     0.977     0.988      6357\n",
      "\n",
      "    accuracy                          0.955     19200\n",
      "   macro avg      0.956     0.955     0.955     19200\n",
      "weighted avg      0.955     0.955     0.955     19200\n",
      "\n",
      "\n",
      "Validation Data Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.945     0.923     0.934      1646\n",
      "           1      0.924     0.965     0.944      1582\n",
      "           2      1.000     0.979     0.989      1572\n",
      "\n",
      "    accuracy                          0.955      4800\n",
      "   macro avg      0.956     0.956     0.956      4800\n",
      "weighted avg      0.956     0.955     0.955      4800\n",
      "\n",
      "\n",
      "Test Data Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.950     0.925     0.938      2050\n",
      "           1      0.927     0.971     0.949      2005\n",
      "           2      1.000     0.979     0.989      1945\n",
      "\n",
      "    accuracy                          0.958      6000\n",
      "   macro avg      0.959     0.958     0.959      6000\n",
      "weighted avg      0.959     0.958     0.958      6000\n",
      "\n",
      "\n",
      "Training dataset size: 24000\n",
      "Test dataset size: 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20126/3419089972.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[\"encoded_label\"] = labelencoder.fit_transform(train['label'])\n",
      "/tmp/ipykernel_20126/3419089972.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test[\"encoded_label\"] = labelencoder.transform(test['label'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Device: NVIDIA GeForce RTX 3070 Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/tmp/ipykernel_20126/3419089972.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_tokens = {key: torch.tensor(val).to(device) for key, val in batch_tokens.items()}\n",
      "/tmp/ipykernel_20126/3419089972.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_tokens = {key: torch.tensor(val).to(device) for key, val in batch_tokens.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape:  torch.Size([19200, 768]) validation data shape:  torch.Size([4800, 768]) test embd shape: torch.Size([6000, 768])\n",
      "Epoch 1/5, Train Loss: 0.1983\n",
      "Epoch 1/5, Validation Loss: 0.0912\n",
      "Epoch 2/5, Train Loss: 0.0905\n",
      "Epoch 2/5, Validation Loss: 0.0801\n",
      "Epoch 3/5, Train Loss: 0.0834\n",
      "Epoch 3/5, Validation Loss: 0.0832\n",
      "Epoch 4/5, Train Loss: 0.0811\n",
      "Epoch 4/5, Validation Loss: 0.0746\n",
      "Epoch 5/5, Train Loss: 0.0806\n",
      "Epoch 5/5, Validation Loss: 0.0786\n",
      "Execution time: 29.62318515777588 seconds\n",
      "Training Data Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.871     0.993     0.928      6506\n",
      "           1      0.991     0.871     0.927      6374\n",
      "           2      1.000     0.978     0.989      6320\n",
      "\n",
      "    accuracy                          0.948     19200\n",
      "   macro avg      0.954     0.947     0.948     19200\n",
      "weighted avg      0.953     0.948     0.948     19200\n",
      "\n",
      "\n",
      "Validation Data Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.883     0.996     0.936      1641\n",
      "           1      0.996     0.887     0.938      1542\n",
      "           2      1.000     0.975     0.987      1617\n",
      "\n",
      "    accuracy                          0.954      4800\n",
      "   macro avg      0.960     0.952     0.954      4800\n",
      "weighted avg      0.959     0.954     0.954      4800\n",
      "\n",
      "\n",
      "Test Data Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.866     0.992     0.925      2033\n",
      "           1      0.991     0.867     0.925      2030\n",
      "           2      1.000     0.978     0.989      1937\n",
      "\n",
      "    accuracy                          0.946      6000\n",
      "   macro avg      0.952     0.946     0.946      6000\n",
      "weighted avg      0.952     0.946     0.946      6000\n",
      "\n",
      "\n",
      "Training dataset size: 24000\n",
      "Test dataset size: 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20126/3419089972.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[\"encoded_label\"] = labelencoder.fit_transform(train['label'])\n",
      "/tmp/ipykernel_20126/3419089972.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test[\"encoded_label\"] = labelencoder.transform(test['label'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Device: NVIDIA GeForce RTX 3070 Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/tmp/ipykernel_20126/3419089972.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_tokens = {key: torch.tensor(val).to(device) for key, val in batch_tokens.items()}\n",
      "/tmp/ipykernel_20126/3419089972.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_tokens = {key: torch.tensor(val).to(device) for key, val in batch_tokens.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape:  torch.Size([19200, 768]) validation data shape:  torch.Size([4800, 768]) test embd shape: torch.Size([6000, 768])\n",
      "Epoch 1/5, Train Loss: 0.2008\n",
      "Epoch 1/5, Validation Loss: 0.0947\n",
      "Epoch 2/5, Train Loss: 0.0892\n",
      "Epoch 2/5, Validation Loss: 0.0855\n",
      "Epoch 3/5, Train Loss: 0.0822\n",
      "Epoch 3/5, Validation Loss: 0.0812\n",
      "Epoch 4/5, Train Loss: 0.0833\n",
      "Epoch 4/5, Validation Loss: 0.0822\n",
      "Epoch 5/5, Train Loss: 0.0801\n",
      "Epoch 5/5, Validation Loss: 0.0919\n",
      "Execution time: 29.411747932434082 seconds\n",
      "Training Data Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.879     0.936      6490\n",
      "           1      0.926     0.966     0.945      6355\n",
      "           2      0.925     1.000     0.961      6355\n",
      "\n",
      "    accuracy                          0.948     19200\n",
      "   macro avg      0.950     0.948     0.947     19200\n",
      "weighted avg      0.951     0.948     0.947     19200\n",
      "\n",
      "\n",
      "Validation Data Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.886     0.939      1612\n",
      "           1      0.926     0.962     0.944      1594\n",
      "           2      0.929     1.000     0.963      1594\n",
      "\n",
      "    accuracy                          0.949      4800\n",
      "   macro avg      0.952     0.949     0.949      4800\n",
      "weighted avg      0.952     0.949     0.949      4800\n",
      "\n",
      "\n",
      "Test Data Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.885     0.939      2078\n",
      "           1      0.922     0.972     0.946      1997\n",
      "           2      0.937     1.000     0.967      1925\n",
      "\n",
      "    accuracy                          0.951      6000\n",
      "   macro avg      0.953     0.952     0.951      6000\n",
      "weighted avg      0.954     0.951     0.950      6000\n",
      "\n",
      "\n",
      "Training dataset size: 24000\n",
      "Test dataset size: 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20126/3419089972.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[\"encoded_label\"] = labelencoder.fit_transform(train['label'])\n",
      "/tmp/ipykernel_20126/3419089972.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test[\"encoded_label\"] = labelencoder.transform(test['label'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Device: NVIDIA GeForce RTX 3070 Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/tmp/ipykernel_20126/3419089972.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_tokens = {key: torch.tensor(val).to(device) for key, val in batch_tokens.items()}\n",
      "/tmp/ipykernel_20126/3419089972.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_tokens = {key: torch.tensor(val).to(device) for key, val in batch_tokens.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape:  torch.Size([19200, 768]) validation data shape:  torch.Size([4800, 768]) test embd shape: torch.Size([6000, 768])\n",
      "Epoch 1/5, Train Loss: 0.1866\n",
      "Epoch 1/5, Validation Loss: 0.0847\n",
      "Epoch 2/5, Train Loss: 0.0900\n",
      "Epoch 2/5, Validation Loss: 0.0761\n",
      "Epoch 3/5, Train Loss: 0.0843\n",
      "Epoch 3/5, Validation Loss: 0.0696\n",
      "Epoch 4/5, Train Loss: 0.0809\n",
      "Epoch 4/5, Validation Loss: 0.0696\n",
      "Epoch 5/5, Train Loss: 0.0845\n",
      "Epoch 5/5, Validation Loss: 0.0711\n",
      "Execution time: 29.246720790863037 seconds\n",
      "Training Data Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.943     0.922     0.933      6536\n",
      "           1      0.923     0.966     0.944      6346\n",
      "           2      1.000     0.977     0.988      6318\n",
      "\n",
      "    accuracy                          0.955     19200\n",
      "   macro avg      0.956     0.955     0.955     19200\n",
      "weighted avg      0.955     0.955     0.955     19200\n",
      "\n",
      "\n",
      "Validation Data Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.932     0.943      1639\n",
      "           1      0.931     0.970     0.950      1559\n",
      "           2      1.000     0.983     0.992      1602\n",
      "\n",
      "    accuracy                          0.961      4800\n",
      "   macro avg      0.962     0.962     0.962      4800\n",
      "weighted avg      0.962     0.961     0.962      4800\n",
      "\n",
      "\n",
      "Test Data Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.939     0.920     0.929      2005\n",
      "           1      0.925     0.965     0.944      2041\n",
      "           2      1.000     0.975     0.988      1954\n",
      "\n",
      "    accuracy                          0.953      6000\n",
      "   macro avg      0.955     0.953     0.954      6000\n",
      "weighted avg      0.954     0.953     0.953      6000\n",
      "\n",
      "\n",
      "Training dataset size: 24000\n",
      "Test dataset size: 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20126/3419089972.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[\"encoded_label\"] = labelencoder.fit_transform(train['label'])\n",
      "/tmp/ipykernel_20126/3419089972.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test[\"encoded_label\"] = labelencoder.transform(test['label'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Device: NVIDIA GeForce RTX 3070 Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/tmp/ipykernel_20126/3419089972.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_tokens = {key: torch.tensor(val).to(device) for key, val in batch_tokens.items()}\n",
      "/tmp/ipykernel_20126/3419089972.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_tokens = {key: torch.tensor(val).to(device) for key, val in batch_tokens.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape:  torch.Size([19200, 768]) validation data shape:  torch.Size([4800, 768]) test embd shape: torch.Size([6000, 768])\n",
      "Epoch 1/5, Train Loss: 0.1836\n",
      "Epoch 1/5, Validation Loss: 0.0907\n",
      "Epoch 2/5, Train Loss: 0.0862\n",
      "Epoch 2/5, Validation Loss: 0.0821\n",
      "Epoch 3/5, Train Loss: 0.0811\n",
      "Epoch 3/5, Validation Loss: 0.0784\n",
      "Epoch 4/5, Train Loss: 0.0786\n",
      "Epoch 4/5, Validation Loss: 0.0793\n",
      "Epoch 5/5, Train Loss: 0.0785\n",
      "Epoch 5/5, Validation Loss: 0.0775\n",
      "Execution time: 29.932053089141846 seconds\n",
      "Training Data Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.943     0.930     0.936      6567\n",
      "           1      0.930     0.963     0.946      6342\n",
      "           2      1.000     0.979     0.989      6291\n",
      "\n",
      "    accuracy                          0.957     19200\n",
      "   macro avg      0.958     0.957     0.957     19200\n",
      "weighted avg      0.957     0.957     0.957     19200\n",
      "\n",
      "\n",
      "Validation Data Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.939     0.922     0.930      1647\n",
      "           1      0.922     0.967     0.944      1556\n",
      "           2      1.000     0.970     0.985      1597\n",
      "\n",
      "    accuracy                          0.953      4800\n",
      "   macro avg      0.953     0.953     0.953      4800\n",
      "weighted avg      0.954     0.953     0.953      4800\n",
      "\n",
      "\n",
      "Test Data Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.941     0.921     0.931      1966\n",
      "           1      0.927     0.963     0.945      2048\n",
      "           2      1.000     0.980     0.990      1986\n",
      "\n",
      "    accuracy                          0.955      6000\n",
      "   macro avg      0.956     0.955     0.955      6000\n",
      "weighted avg      0.956     0.955     0.955      6000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv('insurance_synthetic_data.csv')\n",
    "df  = pd.read_csv('../dataset/insurance_claims_data/insurance_claims.csv')\n",
    "df = df.rename(columns = {'incident_class':'label'})\n",
    "df = df.rename(columns = {'incident_description':'text'})\n",
    "# display(df)\n",
    "print(\"Claim Categories:\", df.label.unique())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PERFORMANCE_RESULTS = pd.DataFrame(columns=['Model', 'EvalSet', 'Accuracy', \n",
    "                       'M-Precision', 'M-Recall', \n",
    "                       'M-F1-Score', 'W-Precision',\n",
    "                       'W-Recall', 'W-F1-Score', 'Runtime'])\n",
    "\n",
    "\n",
    "\n",
    "for KK in list([1,100, 500, 1000, 1500]):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    random.seed(KK)\n",
    "    np.random.seed(KK)\n",
    "    torch.manual_seed(KK)\n",
    "\n",
    "\n",
    "    # Divide the data into train, validation, and test sets\n",
    "    train, test = train_test_split(df, test_size=0.2, random_state=KK)\n",
    "    # val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Print the sizes of the datasets\n",
    "    print(\"Training dataset size:\", len(train))\n",
    "    print(\"Test dataset size:\", len(test))\n",
    "\n",
    "\n",
    "    labelencoder = LabelEncoder()\n",
    "    labelencoder.fit(train['label'])\n",
    "    train[\"encoded_label\"] = labelencoder.fit_transform(train['label'])\n",
    "    test[\"encoded_label\"] = labelencoder.transform(test['label'])\n",
    "    # print(\"Training data Examples: \")\n",
    "    # display(train)\n",
    "    # print(\"Test data Examples: \")\n",
    "    # display(test)\n",
    "\n",
    "\n",
    "\n",
    "    ############################################\n",
    "    ## Data Preprocessing\n",
    "    ############################################\n",
    "\n",
    "\n",
    "    # Convert your train and test text data to lists\n",
    "    train_texts = train[\"text\"].tolist()\n",
    "    test_texts = test[\"text\"].tolist()\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    train_encodings = tokenizer(\n",
    "        train_texts,\n",
    "        padding=True,\n",
    "        max_length=12,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    test_encodings = tokenizer(\n",
    "        test_texts,\n",
    "        padding=True,\n",
    "        max_length=12,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ############################################\n",
    "    ## Extract BERT Embeddings \n",
    "    ############################################\n",
    "\n",
    "    batch_size = 16\n",
    "    train_embeddings = []\n",
    "    test_embeddings = []\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Get the GPU device on which we run our experiments\n",
    "    gpu_name = torch.cuda.get_device_name(device) if device.type == 'cuda' else 'N/A'\n",
    "    print(\"GPU Device:\", gpu_name)\n",
    "\n",
    "\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    model = model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(train_encodings['input_ids']), batch_size):\n",
    "            batch_tokens = {key: val[i:i+batch_size] for key, val in train_encodings.items()}\n",
    "            batch_tokens = {key: torch.tensor(val).to(device) for key, val in batch_tokens.items()}\n",
    "            batch_outputs = model(**batch_tokens)\n",
    "            batch_embeddings = batch_outputs.last_hidden_state[:, 0, :].to('cpu')  # Move back to CPU temporarily\n",
    "            train_embeddings.append(batch_embeddings.to(device))  # Move to CUDA device\n",
    "\n",
    "        for i in range(0, len(test_encodings['input_ids']), batch_size):\n",
    "            batch_tokens = {key: val[i:i+batch_size] for key, val in test_encodings.items()}\n",
    "            batch_tokens = {key: torch.tensor(val).to(device) for key, val in batch_tokens.items()}\n",
    "            batch_outputs = model(**batch_tokens)\n",
    "            batch_embeddings = batch_outputs.last_hidden_state[:, 0, :].to('cpu')  # Move back to CPU temporarily\n",
    "            test_embeddings.append(batch_embeddings.to(device))  # Move to CUDA device\n",
    "\n",
    "    train_embeddings = torch.cat(train_embeddings, dim=0).to('cpu')  # Move back to CPU\n",
    "    test_embeddings = torch.cat(test_embeddings, dim=0).to('cpu')  # Move back to CPU\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ############################################\n",
    "    ## Model Training and Performance Evaluation\n",
    "    ############################################\n",
    "\n",
    "    train_labels=train.encoded_label.to_list()\n",
    "    test_labels=test.encoded_label.to_list()\n",
    "\n",
    "\n",
    "    # Split the data into train and validation sets\n",
    "    train_embeddings1, val_embeddings1, train_labels1, val_labels1 = train_test_split(train_embeddings, \n",
    "                                                                                      train_labels, \n",
    "                                                                                      test_size=0.2, \n",
    "                                                                                      random_state=KK)\n",
    "    print(\"train data shape: \", train_embeddings1.shape, \"validation data shape: \", val_embeddings1.shape,\n",
    "         'test embd shape:', test_embeddings.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Define the multiclass classification model\n",
    "    class MulticlassClassifier(nn.Module):\n",
    "        def __init__(self, input_size, num_classes):\n",
    "            super(MulticlassClassifier, self).__init__()\n",
    "            self.fc1 = nn.Linear(input_size, 128)\n",
    "            self.fc2 = nn.Linear(128, num_classes)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = self.fc1(x)\n",
    "            out = self.relu(out)\n",
    "            out = self.dropout(out)\n",
    "            out = self.fc2(out)\n",
    "            return out\n",
    "\n",
    "    # Initialize the model and optimizer\n",
    "    model_NN = MulticlassClassifier(input_size=train_embeddings1.shape[1], num_classes=len(labelencoder.classes_))\n",
    "    model_NN = model_NN.to(device)  # Move model to CUDA GPU\n",
    "    optimizer = optim.Adam(model_NN.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train the model\n",
    "    num_epochs = 5\n",
    "    batch_size = 16\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for i in range(0, len(train_embeddings1), batch_size):\n",
    "            batch_embeddings = train_embeddings1[i:i+batch_size].requires_grad_(True).to(device)  # Move tensor to CUDA GPU\n",
    "            batch_labels = train_labels1[i:i+batch_size]\n",
    "            batch_labels = torch.tensor(batch_labels).to(device).long()  # Convert to PyTorch tensor and move to CUDA GPU\n",
    "\n",
    "            # Forward pass\n",
    "            train_outputs = model_NN(batch_embeddings)\n",
    "            loss = criterion(train_outputs, batch_labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()  # Accumulate the loss\n",
    "\n",
    "        avg_train_loss = epoch_loss / (len(train_embeddings1) // batch_size)  # Calculate average training loss\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "\n",
    "        # Evaluate the model on validation set\n",
    "        with torch.no_grad():\n",
    "            val_embeddings2 = val_embeddings1.to(device)  # Move tensor to CUDA GPU\n",
    "            val_labels2 = torch.tensor(val_labels1).clone().detach().to(device).long()  # Convert to PyTorch tensor and move to CUDA GPU\n",
    "            val_outputs = model_NN(val_embeddings2)\n",
    "            val_loss = criterion(val_outputs, val_labels2)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time1 = end_time - start_time\n",
    "\n",
    "    print(f\"Execution time: {execution_time1} seconds\")\n",
    "\n",
    "    modname = 'Pretrained_BERTEmbd_with_NN'\n",
    "\n",
    "    # Evaluate the model on training set\n",
    "    model_NN.eval()\n",
    "    with torch.no_grad():\n",
    "        train_emb = train_embeddings1.to(device)  # Move tensor to CUDA GPU\n",
    "        train_lbls = torch.tensor(train_labels1).clone().detach().to(device).long() # Convert to PyTorch tensor and move to CUDA GPU\n",
    "        train_otpt = model_NN(train_emb)\n",
    "\n",
    "\n",
    "    # Convert the predicted values and original test labels to CPU\n",
    "    train_preds = train_otpt.to('cpu').argmax(dim=1).numpy()\n",
    "    train_labels=np.array(train_labels1)\n",
    "\n",
    "    # Generate classification report\n",
    "    print(\"Training Data Classification Report:\")\n",
    "    eval_set='training'\n",
    "    classification_rep_train = classification_report(train_labels, train_preds, digits=3)\n",
    "    print(classification_rep_train)\n",
    "    results_train= export_classification_report(classification_rep_train, modname, execution_time1, eval_set)\n",
    "    PERFORMANCE_RESULTS = pd.concat([PERFORMANCE_RESULTS, results_train])\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Evaluate the model on validation set\n",
    "    model_NN.eval()\n",
    "    with torch.no_grad():\n",
    "        val_emb = val_embeddings1.to(device)  # Move tensor to CUDA GPU\n",
    "        val_lbls = torch.tensor(val_labels1).clone().detach().to(device).long() # Convert to PyTorch tensor and move to CUDA GPU\n",
    "        val_otpt = model_NN(val_emb)\n",
    "    #     test_loss = criterion(val_otpt, val_lbls)    \n",
    "\n",
    "    # Convert the predicted values and original test labels to CPU\n",
    "    val_preds = val_otpt.to('cpu').argmax(dim=1).numpy()\n",
    "    val_labels=np.array(val_labels1)\n",
    "\n",
    "    print(\"Validation Data Classification Report:\")\n",
    "    eval_set='validation'\n",
    "    classification_rep_val = classification_report(val_labels, val_preds, digits=3)\n",
    "    print(classification_rep_val)\n",
    "    results_val= export_classification_report(classification_rep_val, modname, execution_time1, eval_set)\n",
    "    PERFORMANCE_RESULTS = pd.concat([PERFORMANCE_RESULTS, results_val])\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Evaluate the model on test set\n",
    "    model_NN.eval()\n",
    "    with torch.no_grad():\n",
    "        test_embeddings = test_embeddings.to(device)  # Move tensor to CUDA GPU\n",
    "        test_labels = torch.tensor(test_labels).to(device).long()  # Convert to PyTorch tensor and move to CUDA GPU\n",
    "        test_outputs = model_NN(test_embeddings)\n",
    "        test_loss = criterion(test_outputs, test_labels)\n",
    "\n",
    "    # Convert the predicted values and original test labels to CPU\n",
    "    test_preds = test_outputs.to('cpu').argmax(dim=1).numpy()\n",
    "    test_labels = test_labels.to('cpu').numpy()\n",
    "\n",
    "    print(\"Test Data Classification Report:\")\n",
    "    eval_set='test'\n",
    "    classification_rep_test = classification_report(test_labels, test_preds, digits=3)\n",
    "    print(classification_rep_test)\n",
    "    results_test= export_classification_report(classification_rep_test, modname, execution_time1, eval_set)\n",
    "    PERFORMANCE_RESULTS = pd.concat([PERFORMANCE_RESULTS, results_test])\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbe15327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>EvalSet</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>M-Precision</th>\n",
       "      <th>M-Recall</th>\n",
       "      <th>M-F1-Score</th>\n",
       "      <th>W-Precision</th>\n",
       "      <th>W-Recall</th>\n",
       "      <th>W-F1-Score</th>\n",
       "      <th>Runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pretrained_BERTEmbd_with_NN</td>\n",
       "      <td>training</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.955</td>\n",
       "      <td>29.679776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pretrained_BERTEmbd_with_NN</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.955</td>\n",
       "      <td>29.679776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pretrained_BERTEmbd_with_NN</td>\n",
       "      <td>test</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.958</td>\n",
       "      <td>29.679776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pretrained_BERTEmbd_with_NN</td>\n",
       "      <td>training</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.948</td>\n",
       "      <td>29.623185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pretrained_BERTEmbd_with_NN</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.954</td>\n",
       "      <td>29.623185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pretrained_BERTEmbd_with_NN</td>\n",
       "      <td>test</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.946</td>\n",
       "      <td>29.623185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pretrained_BERTEmbd_with_NN</td>\n",
       "      <td>training</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.947</td>\n",
       "      <td>29.411748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pretrained_BERTEmbd_with_NN</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.949</td>\n",
       "      <td>29.411748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pretrained_BERTEmbd_with_NN</td>\n",
       "      <td>test</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.950</td>\n",
       "      <td>29.411748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pretrained_BERTEmbd_with_NN</td>\n",
       "      <td>training</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.955</td>\n",
       "      <td>29.246721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pretrained_BERTEmbd_with_NN</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.962</td>\n",
       "      <td>29.246721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pretrained_BERTEmbd_with_NN</td>\n",
       "      <td>test</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.953</td>\n",
       "      <td>29.246721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pretrained_BERTEmbd_with_NN</td>\n",
       "      <td>training</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.957</td>\n",
       "      <td>29.932053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pretrained_BERTEmbd_with_NN</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.953</td>\n",
       "      <td>29.932053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pretrained_BERTEmbd_with_NN</td>\n",
       "      <td>test</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.955</td>\n",
       "      <td>29.932053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model     EvalSet  Accuracy  M-Precision  M-Recall  \\\n",
       "0  Pretrained_BERTEmbd_with_NN    training     0.955        0.956     0.955   \n",
       "0  Pretrained_BERTEmbd_with_NN  validation     0.955        0.956     0.956   \n",
       "0  Pretrained_BERTEmbd_with_NN        test     0.958        0.959     0.958   \n",
       "0  Pretrained_BERTEmbd_with_NN    training     0.948        0.954     0.947   \n",
       "0  Pretrained_BERTEmbd_with_NN  validation     0.954        0.960     0.952   \n",
       "0  Pretrained_BERTEmbd_with_NN        test     0.946        0.952     0.946   \n",
       "0  Pretrained_BERTEmbd_with_NN    training     0.948        0.950     0.948   \n",
       "0  Pretrained_BERTEmbd_with_NN  validation     0.949        0.952     0.949   \n",
       "0  Pretrained_BERTEmbd_with_NN        test     0.951        0.953     0.952   \n",
       "0  Pretrained_BERTEmbd_with_NN    training     0.955        0.956     0.955   \n",
       "0  Pretrained_BERTEmbd_with_NN  validation     0.961        0.962     0.962   \n",
       "0  Pretrained_BERTEmbd_with_NN        test     0.953        0.955     0.953   \n",
       "0  Pretrained_BERTEmbd_with_NN    training     0.957        0.958     0.957   \n",
       "0  Pretrained_BERTEmbd_with_NN  validation     0.953        0.953     0.953   \n",
       "0  Pretrained_BERTEmbd_with_NN        test     0.955        0.956     0.955   \n",
       "\n",
       "   M-F1-Score  W-Precision  W-Recall  W-F1-Score    Runtime  \n",
       "0       0.955        0.955     0.955       0.955  29.679776  \n",
       "0       0.956        0.956     0.955       0.955  29.679776  \n",
       "0       0.959        0.959     0.958       0.958  29.679776  \n",
       "0       0.948        0.953     0.948       0.948  29.623185  \n",
       "0       0.954        0.959     0.954       0.954  29.623185  \n",
       "0       0.946        0.952     0.946       0.946  29.623185  \n",
       "0       0.947        0.951     0.948       0.947  29.411748  \n",
       "0       0.949        0.952     0.949       0.949  29.411748  \n",
       "0       0.951        0.954     0.951       0.950  29.411748  \n",
       "0       0.955        0.955     0.955       0.955  29.246721  \n",
       "0       0.962        0.962     0.961       0.962  29.246721  \n",
       "0       0.954        0.954     0.953       0.953  29.246721  \n",
       "0       0.957        0.957     0.957       0.957  29.932053  \n",
       "0       0.953        0.954     0.953       0.953  29.932053  \n",
       "0       0.955        0.956     0.955       0.955  29.932053  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PERFORMANCE_RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dab77d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERFORMANCE_RESULTS.to_excel(f'../result_xls/{modname}.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52effc8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
